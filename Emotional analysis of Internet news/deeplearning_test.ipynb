{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉空行应该在数据清理之后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据整合，输出新数据\n",
    "def merge_file(file1,file2):\n",
    "    feature = pd.read_csv(file1,sep=\",\")\n",
    "    label = pd.read_csv(file2,sep=\",\")\n",
    "    data = feature.merge(label,on='id') # 按照id列整合两个训练集表\n",
    "    data[\"X\"] = data[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1) # \"title\",\"content\"两列合为一列\"X\"\n",
    "    df = data[['id', 'title', 'content', 'label', 'X']]\n",
    "    return df\n",
    "\n",
    "\n",
    "data = merge_file(\"Train_DataSet.csv\",\"Train_DataSet_Label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 停用词加载和清洗\n",
    "def stopwords_list(filepath):\n",
    "    stopwords=[line.strip() for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords=stopwords_list('stopwords.txt')\n",
    "\n",
    "def stop_clean(strs):\n",
    "    strs = [w for w in strs if not w in stopwords]\n",
    "    return strs\n",
    "\n",
    "# 只保留汉字\n",
    "def character_save(strs):\n",
    "    strs = re.findall(r'[\\u4e00-\\u9fa5]',strs)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理训练集，将训练集的文本信息和label信息合并，清洗特殊符合，同时将文本内容进行分词\n",
    "def data_pre_handle(df):\n",
    "    \n",
    "    dataDropNa=data\n",
    "    \n",
    "#    dfDropNa[\"X\"]=dataDropNa[\"X\"].apply(lambda x: str(x).replace(\"\\\\n\",\"\").replace(\".\",\"\").replace(\"\\n\",\"\").replace(\"　\",\"\").replace(\"↓\",\"\").replace(\"/\",\"\").replace(\"|\",\"\").replace(\" \",\"\"))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X\"]\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\"\".join(character_save(x)))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\"\".join(stop_clean(x)))\n",
    "    # 程序运行慢是因为stopwords太大\n",
    "    dataDropNa=dataDropNa.dropna(axis=0, how='any')  \n",
    "    # 最后去空值\n",
    "    return dataDropNa\n",
    " \n",
    "data = data_pre_handle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#处理测试数据\n",
    "def process_test(test_name):\n",
    "    test=pd.read_csv(test_name,sep=\",\")\n",
    "    test[\"X\"]=test[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1)\n",
    "    test[\"X_split\"]=test[\"X\"].apply(lambda x:\"\".join(character_save(x)))\n",
    "    test[\"X_split\"]=test[\"X_split\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "    test[\"X_split\"]=test[\"X_split\"].apply(lambda x:\"\".join(stop_clean(x)))\n",
    "    \n",
    "    return test\n",
    " \n",
    "testData=process_test(\"Test_DataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# #处理训练集，将训练集的文本信息和label信息合并，清洗特殊符号，同时将文本内容进行分词\n",
    "# def merge_feature_label(feature_name,label_name):\n",
    "#     feature=pd.read_csv(feature_name,sep=\",\")\n",
    "#     label=pd.read_csv(label_name,sep=\",\")\n",
    "#     data=feature.merge(label,on='id')\n",
    "#     data[\"X\"]=data[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1)\n",
    "#     # 丢掉缺失值\n",
    "#     dataDropNa=data.dropna(axis=0, how='any')\n",
    "#     print(dataDropNa.info())\n",
    "#     dataDropNa[\"X\"]=dataDropNa[\"X\"].apply(lambda x: str(x).replace(\"\\\\n\",\"\").replace(\".\",\"\").replace(\"\\n\",\"\").replace(\"　\",\"\").replace(\"↓\",\"\").replace(\"/\",\"\").replace(\"|\",\"\").replace(\" \",\"\"))\n",
    "#     dataDropNa[\"X_split\"]=dataDropNa[\"X\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "#     return dataDropNa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #处理测试数据\n",
    "# def process_test(test_name):\n",
    "#     test=pd.read_csv(test_name,sep=\",\")\n",
    "#     # apply()方法能劫持另外一个对象的方法，继承另外一个对象的属性 \n",
    "#     # lambda https://blog.csdn.net/zjuxsl/article/details/79437563\n",
    "#     #  join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串。\n",
    "#     test[\"X\"]=test[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1)\n",
    "#     # 列名信息\n",
    "#     print(test.info())\n",
    "#     # replace(old, new) 方法把字符串中的 old（旧字符串） 替换成 new(新字符串)，如果指定第三个参数max，则替换不超过 max 次。\n",
    "#     test[\"X\"]=test[\"X\"].apply(lambda x: str(x).replace(\"\\\\n\",\"\").replace(\".\",\"\").replace(\"\\n\",\"\").replace(\"　\",\"\").replace(\"↓\",\"\").replace(\"/\",\"\").replace(\"|\",\"\").replace(\" \",\"\"))\n",
    "#     # jieba.cut()按照词性切分“我爱中国”切分成我爱中国\n",
    "#     test[\"X_split\"]=test[\"X\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "#     return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataDropNa=merge_feature_label(\"Train_DataSet.csv\",\"Train_DataSet_Label.csv\")\n",
    "dataDropNa = data\n",
    "data_reindex = dataDropNa.reset_index(drop=True)\n",
    "# data_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       问责 领导 上 黄镇 党委书记 张涛 宣国 才 真 一手遮天   天 看  人 举报 施  ...\n",
       "1       江歌 事件 教会 孩子 善良  时 更 懂 保护  去 一年  江歌 悲剧  日 再次 刷 ...\n",
       "2       绝味 鸭 脖 广告 开 黄腔 引 众怒 双  拼值  双 亿  销售额  中国  全世界 感...\n",
       "3       央视 曝光 东 一 医药企业  槽罐车 改成 垃圾车 夜间 偷排 高浓度 废水 年  东 高...\n",
       "4       恶劣 极 央视 都 曝光  南通 东 一 医药企业  槽罐车 改成 洒水车 夜间 偷排 高浓...\n",
       "5       央视 曝光 南通 一 医药企业 夜间 偷排 高浓度 废水 丢脸 昨晚 央视 一套 晚间新闻 ...\n",
       "6       粉丝 爆料 五洲 国际 无锡 项目 涉嫌 诈骗 非法 集资  金融街 号   非法 集资 诈...\n",
       "7       年内 约  锂电 重组 失败 资  高 估值 收购 说 不 摘  中国 电池 联盟  数 显...\n",
       "8       男子 梦想 一夜 暴富 持 水泥块 砸机 一分钱 都 没取  近日 江苏 扬州 谢  盗取 ...\n",
       "9       北京 家 法院 供暖 纠纷案件 主体  供暖费 追缴 山海 网 北京   冬天   年 更 ...\n",
       "10      手机号 开头  注意 看 完   冷汗 都 出  搜 搜狐 原 标题 手机号 开头  注意 ...\n",
       "11      网红 土坯房 书记 落马  背后 原 标题 网红 土坯房 书记 落马  背后 深耕 卢氏 官...\n",
       "12      曾  品质 烤肉 两年    难吃  三鼎 甲 曾  尔滨 烤肉 界 位 显赫 两年 前去 ...\n",
       "13      土俄 军火交易 再 波折 鱼熊 无法 兼顾 土  成美 俄 交锋 牺牲品 搜狐 军事 搜狐网...\n",
       "14        一  闲鱼  网上交易 平台 购买  一 代付款 服务    国外 网站 上 购物   ...\n",
       "15      标题 不 签字 不 意 不 反抗  些 人扮 普天庆  样子 好   新兴 商铺 拆迁  态...\n",
       "16      伙   已 说  教育 废品 文 不 作文 一篇 文章 写  写   不 知道  跑   天...\n",
       "17      车震 视频 外泄 执法  出现 娱乐 化 倾 一度 引发 关注  河北 馆陶 辅警 执法 中...\n",
       "18      全球 警戒 美 官方 称 海外 美国 人  面 恐袭 危险 法治 中国 环球网 综合 报道 ...\n",
       "19      图 信宜市 区 一 男子 疑饮醉 竟濑尿 开 车门 趴 驾驶室 月 日 晚 信宜市 区 南方...\n",
       "20      江歌案  日 开审 真相 便 代价 不必 逢迎 想象 中国 焦点 新闻网 导读 颠末  三 ...\n",
       "21      九江 大桥  举报 空心 桥墩 底  成  机密 专家 沉默 博思网 摘 距 九江 大桥 坍...\n",
       "22      女子 裸贷发 裸 后 没收  钱 反遭 敲诈 还  求 陪 睡 女子 想 裸贷发  裸 后 ...\n",
       "23      马蓉 现身 扮 似 少女  朋友 热聊 儿子  一旁 完全 忽视 令 众人 气愤 导读 原 ...\n",
       "24      忻州 男子 烧 垃圾  罚 元 理 却  奇葩 忻州 男子 烧 垃圾  罚 元 理 却  奇...\n",
       "25      联播 西安 一 饭店 违规 燃放烟花 爆竹 店主  罚款 元 行政拘留 天 陕西 新闻联播 ...\n",
       "26      耶路撒冷 中央 车站 发生 持刀 袭击 事件  时间 号 下午 位 耶路撒冷 市内  中央 ...\n",
       "27      紧急通知 正 看微信   不 轻易 透露 串 数字 马上 告诉 家里人 原 标题 紧急通知 ...\n",
       "28      教科书 式 耍赖 受害  鉴定 车祸  死 家属  追 老赖 刑责 云 备受 关注  教科书...\n",
       "29      袁立手 撕 演员  诞生 节目组 走后门 失败 反帅 锅  小 导演 今天 娱乐圈  上演 ...\n",
       "                              ...                        \n",
       "7235    阴雨  些 危害 安徽 首页 中国 天气 网 中国 天气 网 分钟 前纳特 加强  飓风 已...\n",
       "7236    市中区 王官庄 街道 十 区 社区 发放 消防安全 告知 书 进一步 做好 辖区 火灾 防控...\n",
       "7237    华北 中南部 黄淮 汾渭    霾 南方 区 持续 阴雨 西藏 原 标题 华北 中南部 黄淮...\n",
       "7238    火劫重 生前 女足 球员 流泪 说出  真相 女子 原 标题 火劫重 生前 女足 球员 流泪...\n",
       "7239    开学 一课 观山 湖区 百余 萌娃学 火场 逃生 彩 贵州 网讯 谢静 网 记 杨艳 进一步...\n",
       "7240    看 渭南 渭南 发生  大事 小事 看 里 点击 关注 掌上 渭南 彩 呈现   月 日 阴...\n",
       "7241    中国 百慕大 鄱阳湖 老爷庙 水域 年 吞船 艘  中国 江西省  老爷庙 水域 仅  年代...\n",
       "7242    电力 课堂  线下 违法 建房 亮 红牌 电力设施 保护 条例 十五条  规定  单位  人...\n",
       "7243    台风 少年 团 原  希   村民 一句 话 孩子  失  台风 少年 团 原  希   村...\n",
       "7244       生不出 儿子 豪门 弃妇 最爽 逆袭 老娘 身价 亿原 标题    生不出 儿子 豪门...\n",
       "7245    日 官员 东京 奥运会   人 吃 上 核灾 食品  还 手机 网易 网 列位 欢迎  理说...\n",
       "7246    高伟光 主演  怒 晴 湘西 鹧鸪 哨 背 红 姑娘 爬 上 山崖 奇迹 鹧鸪 哨  红 姑...\n",
       "7247    民间事 老实人  财运 长飞镇   吴 财主 宅子  成片 儿子 坐 一桌 单说  岁 年 ...\n",
       "7248    湖南 水电 职院 召开 赴 沙巴 大学 交流学习 动员会 频道 首页 水利 闻 水利建设 水...\n",
       "7249    平塘 交警 泼  冷水  还 心存 感激  底 回事 老 司机 都 知道 驾驶 大 货车 下...\n",
       "7250    杨紫 爸爸 刺痛 无数 人 父母    死神 间  一堵 墙 点选 设 星标 首席 君 天 ...\n",
       "7251    欲 钱 买 劳怨  动物  什 百度 知道 狗 劳怨 应  主动   不 动  看 报道  ...\n",
       "7252    吉祥 虎 老虎机 最新 平台 吉祥 虎 老虎机 最新 平台 派克   一条条 生命   不 ...\n",
       "7253    广州 学生 遇 暴雨 预警  延迟 上学 不算 迟 旷课 奎屯 新闻网 奎屯 新快报 讯 记...\n",
       "7254    缅甸 赌场 缅甸 赌场 步步 惊心 章 厌烦  只 想 婚 孩儿  回 成绩单 二楼  走廊...\n",
       "7255    帮 女郎  行动 湖北 黄冈 面包车 逆行  卡车 相撞 致一人 困 消防员 成功 救援 月...\n",
       "7256    稻城 亚丁 川藏线 拉萨 青藏线  心动   行程 简介 一阶段  天桥 天 天镇 天 一日...\n",
       "7257    大良 龙江 陈村  学校 月 开学 德 未 两年 力大良 龙江 陈村  学校 月 开学 德 ...\n",
       "7258    最 容易 受 甲醛 侵害  五类 人   污染 原 标题 最 容易 受 甲醛 侵害  五类 ...\n",
       "7259    年 月 事 伤亡 月 报年 月份 全市 生产 安全事 基 情况 年 月份 全市 共 发生 生...\n",
       "7260    珊瑚 裸尾鼠 首  全球 气候 变暖 灭绝  哺乳动物 凯文 登上 前 澳大利亚  飞机 飞...\n",
       "7261    独居 老人 做饭 忘关 火 南通 志愿 时发现 转危安 江海 明珠 网讯 记 修雨竹 汤思敏...\n",
       "7262     生意 上  人  利 合诈骗 诈骗 三十万 够判 少 年  律师 诈骗罪 量刑 标准  加...\n",
       "7263    奎山 汽贸 城 去年 场 火灾 调查 情况 报告 出  日 日 济 技术开发区 发布 关 奎...\n",
       "7264    曝光 台 调查 市场 消防通道  长期 霸占 事情 非 想象  样 消防通道  生命 通道 ...\n",
       "Name: X_split, Length: 7265, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reindex[\"X_split\"][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testData=process_test(\"Test_DataSet.csv\")\n",
    "datatest = testData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'电大    电大   '"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_reindex[\"X_split\"][5287]\n",
    "datatest[\"X_split\"][1407]\n",
    " # 2878 3119 3392 3425 5288\n",
    "    # 1408 3261 6420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_reindex[\"X_split\"][2877] = data_reindex[\"X_split\"][0]\n",
    "data_reindex[\"X_split\"][3118] = data_reindex[\"X_split\"][0]\n",
    "data_reindex[\"X_split\"][3391] = data_reindex[\"X_split\"][0]\n",
    "data_reindex[\"X_split\"][3424] = data_reindex[\"X_split\"][0]\n",
    "data_reindex[\"X_split\"][5287] = data_reindex[\"X_split\"][0]\n",
    "\n",
    "datatest[\"X_split\"][1407] = datatest[\"X_split\"][0]\n",
    "datatest[\"X_split\"][3260] = datatest[\"X_split\"][0]\n",
    "datatest[\"X_split\"][6419] = datatest[\"X_split\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'问责 领导 上 黄镇 党委书记 张涛 宣国 才 真 一手遮天   天 看  人 举报 施   贴子   举报人 联系 证实  宣 天 中午 请 举报人  枪手 喝酒 后 晚上 才 发  贴子 人 不去 讨 前 二天  举报 相信 总 会  说法  今天 一 看施 全军 年月日 实名 举报 上 黄镇 宣国 才  贴子 仍  锁定 禁止 评 已 正好 一 整年  图片 施 全军 实名 举报 天后 上 黄镇 党委政府 回复 下 图 图片 图片 一年  贴子 再次  网友 顶  后 才 发现 施 天 前 回复 网友  处理结果 竟 下 图 图片 现 责问 张涛 书记 宣国 才  举报   问题 什 时候  答复 宣国 才  举报 后 什  立刻 免  村 书记 职务 什   安排  城 队 吃 空响  却 天天   水泥厂 上班 赚 黑钱   月 水泥 吨 近元 纯利润 还 供不应求 宣国 才 还清 上 黄 政府 担保 借 宣国 才 代付 振 东厂 工资 社保  钱    解 宣国 才 占 人 企业 营  欠税 万元 欠 社保 万元 应 还 欠  职工工资 十万 上 黄 政府 算  宣国 才 担保 还 还  厂 合法 会计  老板  判刑 四 六年 现 服刑 厂子  宣国 才 强占 宣国 才 天 赚 万 净利润 却 外 宣称 天天 亏   老板 刑满 回厂 宣国 才  厂 天天 亏   亏  千万元 甚  亿张 涛 书记  承担 还 上 黄 政府 承担 初   亲  厂 交 宣国 才 生产  希 徐 市长 看 贴 后   批示 批示 违建  民生问题 一样 关注 一下  水泥厂    请 徐 市长 抽 日理万机 空 亲 约 谈 一下 事人 特  位 施 站长 千万 不 听取 一面辞'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reindex[\"X_split\"][2877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_to_list(df):\n",
    "    datas = []\n",
    "    for content in df[\"X_split\"][0:]:\n",
    "        dataClean = content.split(\" \")\n",
    "        datas.append(dataClean)\n",
    "    \n",
    "    return datas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_to_list(data_reindex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data_to_list(datatest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7356"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(Y, size=100) # 去掉, iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44056 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s)\n",
    "    words = word_tokenize(words)\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data_reindex.X_split.values \n",
    "y = data_reindex.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_total = data_reindex.X_split.values \n",
    "ytrain_total = data_reindex.label.values\n",
    "xtest_total = datatest.X_split.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7265,)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7265/7265 [00:09<00:00, 765.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7356/7356 [00:09<00:00, 768.58it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_total_w2v  = [sent2vec(x) for x in tqdm(xtrain_total)]\n",
    "xtest_total_w2v  = [sent2vec(x) for x in tqdm(xtest_total)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_total_w2v_np = np.array(xtrain_total_w2v)\n",
    "xtest_total_w2v_np = np.array(xtest_total_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7265, 100)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_total_w2v_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356, 100)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_total_w2v_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = []\n",
    "# i=0\n",
    "# for x in xtrain_total_w2v:\n",
    "    \n",
    "# #     print(x.tolist())\n",
    "#     i +=1\n",
    "#     a += x.tolist()\n",
    "#     if(len(x.tolist())!=100):\n",
    "#         print(len(x.tolist()))\n",
    "#         print(x.tolist())\n",
    "#         print(i)\n",
    "    \n",
    "# #     print(len(x.tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a = []\n",
    "# i=0\n",
    "# for x in xtest_total_w2v:\n",
    "    \n",
    "# #     print(x.tolist())\n",
    "#     i +=1\n",
    "#     a += x.tolist()\n",
    "#     if(len(x.tolist())!=100):\n",
    "#         print(len(x.tolist()))\n",
    "#         print(x.tolist())\n",
    "#         print(i)\n",
    "    \n",
    "# #     print(len(x.tolist()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_total_w2v_np, ytrain_total)\n",
    "predictions = clf.predict_proba(xtest_total_w2v_np)\n",
    "\n",
    "# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7356, 1)\n"
     ]
    }
   ],
   "source": [
    "preds=predictions\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('1014.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_total_w2v_np_scl = scl.fit_transform(xtrain_total_w2v_np)\n",
    "xtest_total_w2v_np_scl = scl.transform(xtest_total_w2v_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain_total_enc = np_utils.to_categorical(ytrain_total)\n",
    "#yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 300 改  100\n",
    "model.add(Dense(300, input_dim=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "# 300 改  100\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "# 14 改  3\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7265/7265 [==============================] - ETA: 36s - loss: 0.28 - ETA: 2s - loss: 0.2759 - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.286 - 1s 88us/step - loss: 0.2864\n",
      "Epoch 2/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.287 - 0s 46us/step - loss: 0.2890\n",
      "Epoch 3/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.291 - 0s 42us/step - loss: 0.2935\n",
      "Epoch 4/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.271 - 0s 41us/step - loss: 0.2802\n",
      "Epoch 5/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.282 - 0s 42us/step - loss: 0.2822\n",
      "Epoch 6/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.282 - 0s 44us/step - loss: 0.2842\n",
      "Epoch 7/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.272 - 0s 41us/step - loss: 0.2746\n",
      "Epoch 8/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.273 - 0s 41us/step - loss: 0.2724\n",
      "Epoch 9/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.263 - 0s 41us/step - loss: 0.2666\n",
      "Epoch 10/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.270 - ETA: 0s - loss: 0.271 - 0s 41us/step - loss: 0.2706\n",
      "Epoch 11/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.257 - 0s 43us/step - loss: 0.2594\n",
      "Epoch 12/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.259 - 0s 44us/step - loss: 0.2606\n",
      "Epoch 13/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.257 - 0s 43us/step - loss: 0.2589\n",
      "Epoch 14/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.247 - 0s 42us/step - loss: 0.2531\n",
      "Epoch 15/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.267 - 0s 50us/step - loss: 0.2705\n",
      "Epoch 16/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.262 - 0s 45us/step - loss: 0.2641\n",
      "Epoch 17/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.244 - 0s 44us/step - loss: 0.2457\n",
      "Epoch 18/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.260 - 0s 42us/step - loss: 0.2604\n",
      "Epoch 19/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.244 - 0s 42us/step - loss: 0.2465\n",
      "Epoch 20/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.229 - 0s 41us/step - loss: 0.2339\n",
      "Epoch 21/50\n",
      "7265/7265 [==============================] - ETA: 1s - loss: 0.191 - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.246 - 0s 42us/step - loss: 0.2491\n",
      "Epoch 22/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.253 - 0s 41us/step - loss: 0.2523\n",
      "Epoch 23/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.251 - 0s 43us/step - loss: 0.2519\n",
      "Epoch 24/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.238 - 0s 43us/step - loss: 0.2405\n",
      "Epoch 25/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.234 - 0s 41us/step - loss: 0.2358\n",
      "Epoch 26/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.241 - 0s 42us/step - loss: 0.2466\n",
      "Epoch 27/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.245 - 0s 41us/step - loss: 0.2456\n",
      "Epoch 28/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.234 - 0s 40us/step - loss: 0.2366\n",
      "Epoch 29/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.244 - 0s 40us/step - loss: 0.2454\n",
      "Epoch 30/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.249 - 0s 47us/step - loss: 0.2488\n",
      "Epoch 31/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.336 - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.238 - ETA: 0s - loss: 0.242 - 0s 41us/step - loss: 0.2404\n",
      "Epoch 32/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.240 - 0s 41us/step - loss: 0.2402\n",
      "Epoch 33/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.228 - 0s 42us/step - loss: 0.2330\n",
      "Epoch 34/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.235 - 0s 42us/step - loss: 0.2385\n",
      "Epoch 35/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.235 - 0s 41us/step - loss: 0.2380\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7265/7265 [==============================] - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.228 - 0s 42us/step - loss: 0.2288\n",
      "Epoch 37/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.199 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.230 - 0s 46us/step - loss: 0.2311\n",
      "Epoch 38/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.230 - 0s 36us/step - loss: 0.2318\n",
      "Epoch 39/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.228 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.230 - 0s 39us/step - loss: 0.2330\n",
      "Epoch 40/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.235 - 0s 38us/step - loss: 0.2361\n",
      "Epoch 41/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.234 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.229 - 0s 38us/step - loss: 0.2281\n",
      "Epoch 42/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.211 - 0s 37us/step - loss: 0.2131\n",
      "Epoch 43/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.222 - 0s 39us/step - loss: 0.2258\n",
      "Epoch 44/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.232 - 0s 38us/step - loss: 0.2323\n",
      "Epoch 45/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.206 - 0s 37us/step - loss: 0.2101\n",
      "Epoch 46/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.224 - 0s 44us/step - loss: 0.2243\n",
      "Epoch 47/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.223 - 0s 44us/step - loss: 0.2246\n",
      "Epoch 48/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.224 - 0s 42us/step - loss: 0.2258\n",
      "Epoch 49/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.387 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.229 - 0s 43us/step - loss: 0.2273\n",
      "Epoch 50/50\n",
      "7265/7265 [==============================] - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.214 - 0s 41us/step - loss: 0.2157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x253990d6be0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(xtrain_total_w2v_np_scl, y=ytrain_total_enc, batch_size=64, \n",
    "          epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7356, 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_proba(xtest_total_w2v_np_scl)\n",
    "preds=predictions\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('0000000050ci.csv',index=None)\n",
    "# 过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_16_input to have shape (100,) but got array with shape (70,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-a775027741da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=200, \n\u001b[1;32m----> 3\u001b[1;33m           verbose=1, callbacks=[earlystop])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_16_input to have shape (100,) but got array with shape (70,)"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=200, \n",
    "          verbose=1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain_total) + list(xtest_total))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain_total)\n",
    "xtest_seq = token.texts_to_sequences(xtest_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 275101/275101 [00:00<00:00, 1145793.13it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Epoch 1/100\n",
      "7265/7265 [==============================] - ETA: 8s - loss: 1.205 - ETA: 6s - loss: 1.119 - ETA: 4s - loss: 1.073 - ETA: 4s - loss: 1.053 - ETA: 3s - loss: 1.034 - ETA: 3s - loss: 1.044 - ETA: 2s - loss: 1.046 - ETA: 2s - loss: 1.047 - ETA: 1s - loss: 1.043 - ETA: 1s - loss: 1.037 - ETA: 1s - loss: 1.031 - ETA: 0s - loss: 1.024 - ETA: 0s - loss: 1.018 - ETA: 0s - loss: 1.013 - 5s 683us/step - loss: 1.0107\n",
      "Epoch 2/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.950 - ETA: 4s - loss: 0.962 - ETA: 3s - loss: 0.955 - ETA: 3s - loss: 0.959 - ETA: 3s - loss: 0.952 - ETA: 2s - loss: 0.947 - ETA: 2s - loss: 0.949 - ETA: 2s - loss: 0.951 - ETA: 1s - loss: 0.952 - ETA: 1s - loss: 0.954 - ETA: 1s - loss: 0.952 - ETA: 0s - loss: 0.950 - ETA: 0s - loss: 0.949 - ETA: 0s - loss: 0.949 - 5s 660us/step - loss: 0.9491\n",
      "Epoch 3/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.940 - ETA: 4s - loss: 0.945 - ETA: 3s - loss: 0.930 - ETA: 3s - loss: 0.938 - ETA: 3s - loss: 0.938 - ETA: 2s - loss: 0.936 - ETA: 2s - loss: 0.933 - ETA: 2s - loss: 0.931 - ETA: 1s - loss: 0.929 - ETA: 1s - loss: 0.928 - ETA: 1s - loss: 0.927 - ETA: 0s - loss: 0.923 - ETA: 0s - loss: 0.924 - ETA: 0s - loss: 0.926 - 5s 713us/step - loss: 0.9247\n",
      "Epoch 4/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.875 - ETA: 4s - loss: 0.885 - ETA: 4s - loss: 0.876 - ETA: 3s - loss: 0.879 - ETA: 3s - loss: 0.888 - ETA: 3s - loss: 0.896 - ETA: 2s - loss: 0.896 - ETA: 2s - loss: 0.899 - ETA: 1s - loss: 0.895 - ETA: 1s - loss: 0.895 - ETA: 1s - loss: 0.896 - ETA: 0s - loss: 0.898 - ETA: 0s - loss: 0.903 - ETA: 0s - loss: 0.902 - 5s 737us/step - loss: 0.9021\n",
      "Epoch 5/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.872 - ETA: 4s - loss: 0.879 - ETA: 4s - loss: 0.868 - ETA: 3s - loss: 0.875 - ETA: 3s - loss: 0.876 - ETA: 3s - loss: 0.879 - ETA: 2s - loss: 0.877 - ETA: 2s - loss: 0.874 - ETA: 1s - loss: 0.878 - ETA: 1s - loss: 0.880 - ETA: 1s - loss: 0.879 - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.880 - ETA: 0s - loss: 0.879 - 6s 761us/step - loss: 0.8809\n",
      "Epoch 6/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.820 - ETA: 4s - loss: 0.881 - ETA: 4s - loss: 0.873 - ETA: 3s - loss: 0.867 - ETA: 3s - loss: 0.872 - ETA: 3s - loss: 0.870 - ETA: 2s - loss: 0.867 - ETA: 2s - loss: 0.868 - ETA: 2s - loss: 0.866 - ETA: 1s - loss: 0.866 - ETA: 1s - loss: 0.864 - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.868 - ETA: 0s - loss: 0.870 - 6s 829us/step - loss: 0.8700\n",
      "Epoch 7/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.897 - ETA: 4s - loss: 0.870 - ETA: 4s - loss: 0.868 - ETA: 4s - loss: 0.869 - ETA: 3s - loss: 0.861 - ETA: 3s - loss: 0.856 - ETA: 3s - loss: 0.858 - ETA: 2s - loss: 0.857 - ETA: 2s - loss: 0.858 - ETA: 1s - loss: 0.855 - ETA: 1s - loss: 0.853 - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.847 - 6s 873us/step - loss: 0.8491\n",
      "Epoch 8/100\n",
      "7265/7265 [==============================] - ETA: 6s - loss: 0.877 - ETA: 5s - loss: 0.845 - ETA: 5s - loss: 0.850 - ETA: 4s - loss: 0.845 - ETA: 4s - loss: 0.845 - ETA: 3s - loss: 0.844 - ETA: 3s - loss: 0.841 - ETA: 2s - loss: 0.840 - ETA: 2s - loss: 0.837 - ETA: 1s - loss: 0.836 - ETA: 1s - loss: 0.840 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.843 - 6s 874us/step - loss: 0.8428\n",
      "Epoch 9/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.833 - ETA: 4s - loss: 0.835 - ETA: 4s - loss: 0.829 - ETA: 4s - loss: 0.829 - ETA: 3s - loss: 0.840 - ETA: 3s - loss: 0.834 - ETA: 3s - loss: 0.835 - ETA: 2s - loss: 0.836 - ETA: 2s - loss: 0.839 - ETA: 1s - loss: 0.835 - ETA: 1s - loss: 0.834 - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.829 - 6s 834us/step - loss: 0.8291\n",
      "Epoch 10/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.808 - ETA: 5s - loss: 0.810 - ETA: 4s - loss: 0.825 - ETA: 4s - loss: 0.831 - ETA: 3s - loss: 0.828 - ETA: 3s - loss: 0.835 - ETA: 2s - loss: 0.838 - ETA: 2s - loss: 0.836 - ETA: 2s - loss: 0.836 - ETA: 1s - loss: 0.833 - ETA: 1s - loss: 0.832 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.827 - 6s 827us/step - loss: 0.8277\n",
      "Epoch 11/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.845 - ETA: 4s - loss: 0.844 - ETA: 4s - loss: 0.835 - ETA: 4s - loss: 0.830 - ETA: 3s - loss: 0.824 - ETA: 3s - loss: 0.830 - ETA: 2s - loss: 0.835 - ETA: 2s - loss: 0.835 - ETA: 2s - loss: 0.835 - ETA: 1s - loss: 0.830 - ETA: 1s - loss: 0.830 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.825 - ETA: 0s - loss: 0.822 - 6s 817us/step - loss: 0.8218\n",
      "Epoch 12/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.788 - ETA: 5s - loss: 0.817 - ETA: 4s - loss: 0.817 - ETA: 4s - loss: 0.818 - ETA: 3s - loss: 0.818 - ETA: 3s - loss: 0.815 - ETA: 3s - loss: 0.814 - ETA: 2s - loss: 0.810 - ETA: 2s - loss: 0.812 - ETA: 1s - loss: 0.814 - ETA: 1s - loss: 0.811 - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.811 - 6s 817us/step - loss: 0.8112\n",
      "Epoch 13/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.788 - ETA: 5s - loss: 0.801 - ETA: 4s - loss: 0.809 - ETA: 4s - loss: 0.807 - ETA: 3s - loss: 0.801 - ETA: 3s - loss: 0.806 - ETA: 2s - loss: 0.798 - ETA: 2s - loss: 0.794 - ETA: 2s - loss: 0.797 - ETA: 1s - loss: 0.802 - ETA: 1s - loss: 0.804 - ETA: 0s - loss: 0.808 - ETA: 0s - loss: 0.807 - ETA: 0s - loss: 0.811 - 6s 789us/step - loss: 0.8115\n",
      "Epoch 14/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.805 - ETA: 4s - loss: 0.802 - ETA: 4s - loss: 0.802 - ETA: 4s - loss: 0.801 - ETA: 3s - loss: 0.795 - ETA: 3s - loss: 0.798 - ETA: 2s - loss: 0.797 - ETA: 2s - loss: 0.796 - ETA: 2s - loss: 0.797 - ETA: 1s - loss: 0.798 - ETA: 1s - loss: 0.794 - ETA: 0s - loss: 0.793 - ETA: 0s - loss: 0.794 - ETA: 0s - loss: 0.797 - 6s 815us/step - loss: 0.7965\n",
      "Epoch 15/100\n",
      "7265/7265 [==============================] - ETA: 6s - loss: 0.844 - ETA: 5s - loss: 0.833 - ETA: 4s - loss: 0.804 - ETA: 4s - loss: 0.806 - ETA: 3s - loss: 0.805 - ETA: 3s - loss: 0.809 - ETA: 2s - loss: 0.800 - ETA: 2s - loss: 0.798 - ETA: 2s - loss: 0.796 - ETA: 1s - loss: 0.790 - ETA: 1s - loss: 0.789 - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.790 - 6s 805us/step - loss: 0.7893\n",
      "Epoch 16/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.825 - ETA: 4s - loss: 0.811 - ETA: 4s - loss: 0.802 - ETA: 4s - loss: 0.797 - ETA: 3s - loss: 0.785 - ETA: 3s - loss: 0.788 - ETA: 2s - loss: 0.788 - ETA: 2s - loss: 0.795 - ETA: 2s - loss: 0.793 - ETA: 1s - loss: 0.790 - ETA: 1s - loss: 0.789 - ETA: 0s - loss: 0.789 - ETA: 0s - loss: 0.792 - ETA: 0s - loss: 0.795 - 6s 787us/step - loss: 0.7946\n",
      "Epoch 17/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.794 - ETA: 5s - loss: 0.780 - ETA: 4s - loss: 0.786 - ETA: 4s - loss: 0.776 - ETA: 3s - loss: 0.768 - ETA: 3s - loss: 0.767 - ETA: 2s - loss: 0.769 - ETA: 2s - loss: 0.773 - ETA: 2s - loss: 0.780 - ETA: 1s - loss: 0.783 - ETA: 1s - loss: 0.785 - ETA: 0s - loss: 0.784 - ETA: 0s - loss: 0.786 - ETA: 0s - loss: 0.787 - 6s 788us/step - loss: 0.7869\n",
      "Epoch 18/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.728 - ETA: 4s - loss: 0.764 - ETA: 4s - loss: 0.779 - ETA: 4s - loss: 0.765 - ETA: 3s - loss: 0.767 - ETA: 3s - loss: 0.773 - ETA: 2s - loss: 0.770 - ETA: 2s - loss: 0.769 - ETA: 2s - loss: 0.769 - ETA: 1s - loss: 0.772 - ETA: 1s - loss: 0.770 - ETA: 0s - loss: 0.775 - ETA: 0s - loss: 0.778 - ETA: 0s - loss: 0.775 - 6s 805us/step - loss: 0.7769\n",
      "Epoch 19/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.763 - ETA: 5s - loss: 0.787 - ETA: 4s - loss: 0.775 - ETA: 4s - loss: 0.775 - ETA: 3s - loss: 0.774 - ETA: 3s - loss: 0.779 - ETA: 2s - loss: 0.773 - ETA: 2s - loss: 0.770 - ETA: 2s - loss: 0.775 - ETA: 1s - loss: 0.778 - ETA: 1s - loss: 0.777 - ETA: 0s - loss: 0.775 - ETA: 0s - loss: 0.776 - ETA: 0s - loss: 0.777 - 6s 840us/step - loss: 0.7765\n",
      "Epoch 20/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.781 - ETA: 4s - loss: 0.761 - ETA: 4s - loss: 0.758 - ETA: 4s - loss: 0.766 - ETA: 3s - loss: 0.771 - ETA: 3s - loss: 0.777 - ETA: 2s - loss: 0.777 - ETA: 2s - loss: 0.774 - ETA: 2s - loss: 0.775 - ETA: 1s - loss: 0.776 - ETA: 1s - loss: 0.780 - ETA: 0s - loss: 0.781 - ETA: 0s - loss: 0.776 - ETA: 0s - loss: 0.774 - 6s 804us/step - loss: 0.7745\n",
      "Epoch 21/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.768 - ETA: 5s - loss: 0.761 - ETA: 4s - loss: 0.757 - ETA: 4s - loss: 0.763 - ETA: 3s - loss: 0.767 - ETA: 3s - loss: 0.773 - ETA: 2s - loss: 0.769 - ETA: 2s - loss: 0.775 - ETA: 2s - loss: 0.778 - ETA: 1s - loss: 0.773 - ETA: 1s - loss: 0.775 - ETA: 0s - loss: 0.773 - ETA: 0s - loss: 0.773 - ETA: 0s - loss: 0.772 - 6s 797us/step - loss: 0.7740\n",
      "Epoch 22/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.773 - ETA: 4s - loss: 0.798 - ETA: 4s - loss: 0.791 - ETA: 4s - loss: 0.778 - ETA: 3s - loss: 0.776 - ETA: 3s - loss: 0.774 - ETA: 2s - loss: 0.775 - ETA: 2s - loss: 0.768 - ETA: 2s - loss: 0.766 - ETA: 1s - loss: 0.763 - ETA: 1s - loss: 0.763 - ETA: 0s - loss: 0.763 - ETA: 0s - loss: 0.766 - ETA: 0s - loss: 0.766 - 6s 773us/step - loss: 0.7659\n",
      "Epoch 23/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.682 - ETA: 4s - loss: 0.721 - ETA: 4s - loss: 0.721 - ETA: 3s - loss: 0.728 - ETA: 3s - loss: 0.743 - ETA: 3s - loss: 0.748 - ETA: 2s - loss: 0.757 - ETA: 2s - loss: 0.759 - ETA: 2s - loss: 0.757 - ETA: 1s - loss: 0.760 - ETA: 1s - loss: 0.760 - ETA: 0s - loss: 0.762 - ETA: 0s - loss: 0.763 - ETA: 0s - loss: 0.763 - 6s 778us/step - loss: 0.7637\n",
      "Epoch 24/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.731 - ETA: 5s - loss: 0.743 - ETA: 4s - loss: 0.744 - ETA: 4s - loss: 0.753 - ETA: 3s - loss: 0.754 - ETA: 3s - loss: 0.758 - ETA: 2s - loss: 0.766 - ETA: 2s - loss: 0.764 - ETA: 2s - loss: 0.767 - ETA: 1s - loss: 0.769 - ETA: 1s - loss: 0.769 - ETA: 0s - loss: 0.768 - ETA: 0s - loss: 0.764 - ETA: 0s - loss: 0.764 - 6s 784us/step - loss: 0.7642\n",
      "Epoch 25/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.777 - ETA: 4s - loss: 0.758 - ETA: 4s - loss: 0.768 - ETA: 4s - loss: 0.755 - ETA: 3s - loss: 0.752 - ETA: 3s - loss: 0.748 - ETA: 2s - loss: 0.752 - ETA: 2s - loss: 0.756 - ETA: 2s - loss: 0.757 - ETA: 1s - loss: 0.756 - ETA: 1s - loss: 0.756 - ETA: 0s - loss: 0.750 - ETA: 0s - loss: 0.754 - ETA: 0s - loss: 0.756 - 6s 795us/step - loss: 0.7549\n",
      "Epoch 26/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.743 - ETA: 5s - loss: 0.743 - ETA: 4s - loss: 0.762 - ETA: 4s - loss: 0.761 - ETA: 3s - loss: 0.754 - ETA: 3s - loss: 0.759 - ETA: 2s - loss: 0.754 - ETA: 2s - loss: 0.753 - ETA: 2s - loss: 0.751 - ETA: 1s - loss: 0.753 - ETA: 1s - loss: 0.752 - ETA: 0s - loss: 0.755 - ETA: 0s - loss: 0.755 - ETA: 0s - loss: 0.754 - 6s 814us/step - loss: 0.7541\n",
      "Epoch 27/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.790 - ETA: 4s - loss: 0.774 - ETA: 4s - loss: 0.762 - ETA: 3s - loss: 0.754 - ETA: 3s - loss: 0.764 - ETA: 3s - loss: 0.758 - ETA: 2s - loss: 0.757 - ETA: 2s - loss: 0.752 - ETA: 2s - loss: 0.750 - ETA: 1s - loss: 0.751 - ETA: 1s - loss: 0.754 - ETA: 0s - loss: 0.756 - ETA: 0s - loss: 0.754 - ETA: 0s - loss: 0.752 - 6s 791us/step - loss: 0.7533\n",
      "Epoch 28/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.735 - ETA: 4s - loss: 0.772 - ETA: 4s - loss: 0.770 - ETA: 4s - loss: 0.769 - ETA: 3s - loss: 0.758 - ETA: 3s - loss: 0.759 - ETA: 2s - loss: 0.760 - ETA: 2s - loss: 0.757 - ETA: 2s - loss: 0.757 - ETA: 1s - loss: 0.755 - ETA: 1s - loss: 0.753 - ETA: 0s - loss: 0.748 - ETA: 0s - loss: 0.748 - ETA: 0s - loss: 0.748 - 6s 808us/step - loss: 0.7489\n",
      "Epoch 29/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.717 - ETA: 4s - loss: 0.711 - ETA: 4s - loss: 0.717 - ETA: 4s - loss: 0.731 - ETA: 3s - loss: 0.735 - ETA: 3s - loss: 0.740 - ETA: 2s - loss: 0.739 - ETA: 2s - loss: 0.736 - ETA: 2s - loss: 0.735 - ETA: 1s - loss: 0.733 - ETA: 1s - loss: 0.735 - ETA: 0s - loss: 0.738 - ETA: 0s - loss: 0.741 - ETA: 0s - loss: 0.742 - 6s 790us/step - loss: 0.7401\n",
      "Epoch 30/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.762 - ETA: 5s - loss: 0.777 - ETA: 4s - loss: 0.759 - ETA: 4s - loss: 0.766 - ETA: 3s - loss: 0.754 - ETA: 3s - loss: 0.752 - ETA: 2s - loss: 0.747 - ETA: 2s - loss: 0.741 - ETA: 2s - loss: 0.740 - ETA: 1s - loss: 0.737 - ETA: 1s - loss: 0.739 - ETA: 0s - loss: 0.742 - ETA: 0s - loss: 0.744 - ETA: 0s - loss: 0.743 - 6s 805us/step - loss: 0.7436\n",
      "Epoch 31/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.750 - ETA: 5s - loss: 0.741 - ETA: 4s - loss: 0.740 - ETA: 4s - loss: 0.733 - ETA: 3s - loss: 0.739 - ETA: 3s - loss: 0.736 - ETA: 3s - loss: 0.740 - ETA: 2s - loss: 0.741 - ETA: 2s - loss: 0.736 - ETA: 1s - loss: 0.737 - ETA: 1s - loss: 0.738 - ETA: 0s - loss: 0.739 - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.742 - 6s 840us/step - loss: 0.7426\n",
      "Epoch 32/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.769 - ETA: 4s - loss: 0.763 - ETA: 4s - loss: 0.748 - ETA: 4s - loss: 0.749 - ETA: 3s - loss: 0.736 - ETA: 3s - loss: 0.737 - ETA: 3s - loss: 0.736 - ETA: 2s - loss: 0.733 - ETA: 2s - loss: 0.737 - ETA: 1s - loss: 0.735 - ETA: 1s - loss: 0.735 - ETA: 0s - loss: 0.736 - ETA: 0s - loss: 0.731 - ETA: 0s - loss: 0.731 - 6s 818us/step - loss: 0.7322\n",
      "Epoch 33/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.730 - ETA: 4s - loss: 0.716 - ETA: 4s - loss: 0.724 - ETA: 3s - loss: 0.720 - ETA: 3s - loss: 0.726 - ETA: 3s - loss: 0.736 - ETA: 2s - loss: 0.734 - ETA: 2s - loss: 0.736 - ETA: 2s - loss: 0.737 - ETA: 1s - loss: 0.740 - ETA: 1s - loss: 0.741 - ETA: 0s - loss: 0.743 - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.741 - 6s 779us/step - loss: 0.7413\n",
      "Epoch 34/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.759 - ETA: 4s - loss: 0.759 - ETA: 4s - loss: 0.746 - ETA: 3s - loss: 0.750 - ETA: 3s - loss: 0.745 - ETA: 3s - loss: 0.741 - ETA: 2s - loss: 0.745 - ETA: 2s - loss: 0.747 - ETA: 2s - loss: 0.748 - ETA: 1s - loss: 0.739 - ETA: 1s - loss: 0.735 - ETA: 0s - loss: 0.732 - ETA: 0s - loss: 0.730 - ETA: 0s - loss: 0.730 - 6s 808us/step - loss: 0.7331\n",
      "Epoch 35/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.706 - ETA: 5s - loss: 0.756 - ETA: 4s - loss: 0.745 - ETA: 4s - loss: 0.743 - ETA: 3s - loss: 0.733 - ETA: 3s - loss: 0.740 - ETA: 2s - loss: 0.746 - ETA: 2s - loss: 0.743 - ETA: 2s - loss: 0.745 - ETA: 1s - loss: 0.740 - ETA: 1s - loss: 0.740 - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.741 - ETA: 0s - loss: 0.741 - 6s 794us/step - loss: 0.7414\n",
      "Epoch 36/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.777 - ETA: 5s - loss: 0.727 - ETA: 4s - loss: 0.720 - ETA: 4s - loss: 0.724 - ETA: 3s - loss: 0.725 - ETA: 3s - loss: 0.715 - ETA: 3s - loss: 0.717 - ETA: 2s - loss: 0.727 - ETA: 2s - loss: 0.726 - ETA: 1s - loss: 0.727 - ETA: 1s - loss: 0.723 - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.723 - ETA: 0s - loss: 0.728 - 6s 798us/step - loss: 0.7276\n",
      "Epoch 37/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.741 - ETA: 5s - loss: 0.739 - ETA: 4s - loss: 0.737 - ETA: 4s - loss: 0.736 - ETA: 3s - loss: 0.737 - ETA: 3s - loss: 0.729 - ETA: 2s - loss: 0.722 - ETA: 2s - loss: 0.730 - ETA: 2s - loss: 0.725 - ETA: 1s - loss: 0.724 - ETA: 1s - loss: 0.730 - ETA: 0s - loss: 0.727 - ETA: 0s - loss: 0.728 - ETA: 0s - loss: 0.729 - 6s 807us/step - loss: 0.7292\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7265/7265 [==============================] - ETA: 4s - loss: 0.772 - ETA: 4s - loss: 0.745 - ETA: 4s - loss: 0.732 - ETA: 4s - loss: 0.735 - ETA: 3s - loss: 0.741 - ETA: 3s - loss: 0.733 - ETA: 2s - loss: 0.732 - ETA: 2s - loss: 0.726 - ETA: 2s - loss: 0.723 - ETA: 1s - loss: 0.724 - ETA: 1s - loss: 0.719 - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.720 - ETA: 0s - loss: 0.723 - 6s 789us/step - loss: 0.7223\n",
      "Epoch 39/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.727 - ETA: 5s - loss: 0.731 - ETA: 4s - loss: 0.737 - ETA: 3s - loss: 0.728 - ETA: 3s - loss: 0.733 - ETA: 3s - loss: 0.725 - ETA: 2s - loss: 0.723 - ETA: 2s - loss: 0.723 - ETA: 2s - loss: 0.723 - ETA: 1s - loss: 0.720 - ETA: 1s - loss: 0.720 - ETA: 0s - loss: 0.718 - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.721 - 6s 796us/step - loss: 0.7199\n",
      "Epoch 40/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.724 - ETA: 4s - loss: 0.735 - ETA: 4s - loss: 0.711 - ETA: 4s - loss: 0.716 - ETA: 3s - loss: 0.720 - ETA: 3s - loss: 0.718 - ETA: 2s - loss: 0.720 - ETA: 2s - loss: 0.720 - ETA: 2s - loss: 0.721 - ETA: 1s - loss: 0.725 - ETA: 1s - loss: 0.721 - ETA: 0s - loss: 0.718 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.716 - 6s 809us/step - loss: 0.7151\n",
      "Epoch 41/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.725 - ETA: 4s - loss: 0.726 - ETA: 4s - loss: 0.727 - ETA: 4s - loss: 0.718 - ETA: 3s - loss: 0.718 - ETA: 3s - loss: 0.713 - ETA: 2s - loss: 0.717 - ETA: 2s - loss: 0.716 - ETA: 2s - loss: 0.720 - ETA: 1s - loss: 0.721 - ETA: 1s - loss: 0.718 - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.723 - ETA: 0s - loss: 0.721 - 6s 794us/step - loss: 0.7223\n",
      "Epoch 42/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.772 - ETA: 4s - loss: 0.728 - ETA: 4s - loss: 0.745 - ETA: 3s - loss: 0.738 - ETA: 3s - loss: 0.725 - ETA: 3s - loss: 0.720 - ETA: 2s - loss: 0.718 - ETA: 2s - loss: 0.721 - ETA: 2s - loss: 0.721 - ETA: 1s - loss: 0.718 - ETA: 1s - loss: 0.718 - ETA: 0s - loss: 0.721 - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.721 - 6s 778us/step - loss: 0.7222\n",
      "Epoch 43/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.729 - ETA: 4s - loss: 0.737 - ETA: 4s - loss: 0.733 - ETA: 4s - loss: 0.723 - ETA: 3s - loss: 0.717 - ETA: 3s - loss: 0.716 - ETA: 2s - loss: 0.714 - ETA: 2s - loss: 0.714 - ETA: 2s - loss: 0.709 - ETA: 1s - loss: 0.708 - ETA: 1s - loss: 0.711 - ETA: 0s - loss: 0.713 - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.712 - 6s 797us/step - loss: 0.7144\n",
      "Epoch 44/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.713 - ETA: 5s - loss: 0.721 - ETA: 4s - loss: 0.704 - ETA: 4s - loss: 0.696 - ETA: 3s - loss: 0.701 - ETA: 3s - loss: 0.700 - ETA: 2s - loss: 0.703 - ETA: 2s - loss: 0.698 - ETA: 2s - loss: 0.699 - ETA: 1s - loss: 0.699 - ETA: 1s - loss: 0.706 - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.713 - 6s 809us/step - loss: 0.7134\n",
      "Epoch 45/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.699 - ETA: 4s - loss: 0.721 - ETA: 4s - loss: 0.726 - ETA: 4s - loss: 0.727 - ETA: 3s - loss: 0.724 - ETA: 3s - loss: 0.730 - ETA: 2s - loss: 0.725 - ETA: 2s - loss: 0.717 - ETA: 2s - loss: 0.711 - ETA: 1s - loss: 0.714 - ETA: 1s - loss: 0.713 - ETA: 0s - loss: 0.712 - ETA: 0s - loss: 0.711 - ETA: 0s - loss: 0.713 - 6s 813us/step - loss: 0.7146\n",
      "Epoch 46/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.714 - ETA: 5s - loss: 0.677 - ETA: 4s - loss: 0.685 - ETA: 4s - loss: 0.685 - ETA: 3s - loss: 0.683 - ETA: 3s - loss: 0.690 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.696 - ETA: 1s - loss: 0.700 - ETA: 1s - loss: 0.702 - ETA: 0s - loss: 0.701 - ETA: 0s - loss: 0.702 - ETA: 0s - loss: 0.706 - 6s 807us/step - loss: 0.7064\n",
      "Epoch 47/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.647 - ETA: 4s - loss: 0.654 - ETA: 4s - loss: 0.666 - ETA: 4s - loss: 0.692 - ETA: 3s - loss: 0.704 - ETA: 3s - loss: 0.706 - ETA: 2s - loss: 0.710 - ETA: 2s - loss: 0.710 - ETA: 2s - loss: 0.705 - ETA: 1s - loss: 0.699 - ETA: 1s - loss: 0.702 - ETA: 0s - loss: 0.704 - ETA: 0s - loss: 0.706 - ETA: 0s - loss: 0.704 - 6s 794us/step - loss: 0.7043\n",
      "Epoch 48/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.706 - ETA: 5s - loss: 0.683 - ETA: 4s - loss: 0.679 - ETA: 4s - loss: 0.682 - ETA: 3s - loss: 0.675 - ETA: 3s - loss: 0.684 - ETA: 2s - loss: 0.692 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.695 - ETA: 1s - loss: 0.698 - ETA: 1s - loss: 0.696 - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.703 - ETA: 0s - loss: 0.703 - 6s 807us/step - loss: 0.7013\n",
      "Epoch 49/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.741 - ETA: 4s - loss: 0.734 - ETA: 4s - loss: 0.737 - ETA: 4s - loss: 0.739 - ETA: 3s - loss: 0.738 - ETA: 3s - loss: 0.728 - ETA: 2s - loss: 0.719 - ETA: 2s - loss: 0.716 - ETA: 2s - loss: 0.715 - ETA: 1s - loss: 0.712 - ETA: 1s - loss: 0.706 - ETA: 0s - loss: 0.704 - ETA: 0s - loss: 0.702 - ETA: 0s - loss: 0.701 - 6s 799us/step - loss: 0.7027\n",
      "Epoch 50/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.740 - ETA: 4s - loss: 0.713 - ETA: 4s - loss: 0.715 - ETA: 4s - loss: 0.711 - ETA: 3s - loss: 0.705 - ETA: 3s - loss: 0.708 - ETA: 2s - loss: 0.696 - ETA: 2s - loss: 0.695 - ETA: 2s - loss: 0.694 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.694 - ETA: 0s - loss: 0.696 - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.693 - 6s 804us/step - loss: 0.6928\n",
      "Epoch 51/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.698 - ETA: 4s - loss: 0.682 - ETA: 4s - loss: 0.672 - ETA: 3s - loss: 0.669 - ETA: 3s - loss: 0.679 - ETA: 3s - loss: 0.690 - ETA: 2s - loss: 0.687 - ETA: 2s - loss: 0.695 - ETA: 2s - loss: 0.698 - ETA: 1s - loss: 0.701 - ETA: 1s - loss: 0.697 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.700 - 6s 794us/step - loss: 0.7003\n",
      "Epoch 52/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.779 - ETA: 5s - loss: 0.738 - ETA: 4s - loss: 0.732 - ETA: 4s - loss: 0.708 - ETA: 3s - loss: 0.706 - ETA: 3s - loss: 0.703 - ETA: 2s - loss: 0.707 - ETA: 2s - loss: 0.705 - ETA: 2s - loss: 0.698 - ETA: 1s - loss: 0.692 - ETA: 1s - loss: 0.694 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.691 - ETA: 0s - loss: 0.691 - 6s 796us/step - loss: 0.6905\n",
      "Epoch 53/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.679 - ETA: 4s - loss: 0.661 - ETA: 4s - loss: 0.666 - ETA: 4s - loss: 0.666 - ETA: 3s - loss: 0.679 - ETA: 3s - loss: 0.690 - ETA: 2s - loss: 0.690 - ETA: 2s - loss: 0.689 - ETA: 2s - loss: 0.690 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.696 - ETA: 0s - loss: 0.699 - 6s 794us/step - loss: 0.6987\n",
      "Epoch 54/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.690 - ETA: 4s - loss: 0.691 - ETA: 4s - loss: 0.695 - ETA: 4s - loss: 0.704 - ETA: 3s - loss: 0.705 - ETA: 3s - loss: 0.696 - ETA: 2s - loss: 0.690 - ETA: 2s - loss: 0.690 - ETA: 2s - loss: 0.689 - ETA: 1s - loss: 0.686 - ETA: 1s - loss: 0.687 - ETA: 0s - loss: 0.689 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.689 - 6s 800us/step - loss: 0.6889\n",
      "Epoch 55/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.684 - ETA: 4s - loss: 0.680 - ETA: 4s - loss: 0.674 - ETA: 4s - loss: 0.686 - ETA: 3s - loss: 0.681 - ETA: 3s - loss: 0.676 - ETA: 2s - loss: 0.682 - ETA: 2s - loss: 0.683 - ETA: 2s - loss: 0.686 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.692 - ETA: 0s - loss: 0.692 - ETA: 0s - loss: 0.690 - ETA: 0s - loss: 0.688 - 6s 797us/step - loss: 0.6884\n",
      "Epoch 56/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.725 - ETA: 4s - loss: 0.698 - ETA: 4s - loss: 0.696 - ETA: 4s - loss: 0.697 - ETA: 3s - loss: 0.688 - ETA: 3s - loss: 0.689 - ETA: 2s - loss: 0.692 - ETA: 2s - loss: 0.689 - ETA: 2s - loss: 0.692 - ETA: 1s - loss: 0.688 - ETA: 1s - loss: 0.683 - ETA: 0s - loss: 0.683 - ETA: 0s - loss: 0.683 - ETA: 0s - loss: 0.683 - 6s 805us/step - loss: 0.6828\n",
      "Epoch 57/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.692 - ETA: 5s - loss: 0.682 - ETA: 4s - loss: 0.654 - ETA: 4s - loss: 0.660 - ETA: 3s - loss: 0.653 - ETA: 3s - loss: 0.659 - ETA: 2s - loss: 0.658 - ETA: 2s - loss: 0.668 - ETA: 2s - loss: 0.669 - ETA: 1s - loss: 0.668 - ETA: 1s - loss: 0.671 - ETA: 0s - loss: 0.678 - ETA: 0s - loss: 0.679 - ETA: 0s - loss: 0.683 - 6s 816us/step - loss: 0.6841\n",
      "Epoch 58/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.718 - ETA: 4s - loss: 0.709 - ETA: 4s - loss: 0.687 - ETA: 4s - loss: 0.684 - ETA: 3s - loss: 0.695 - ETA: 3s - loss: 0.690 - ETA: 2s - loss: 0.682 - ETA: 2s - loss: 0.676 - ETA: 2s - loss: 0.679 - ETA: 1s - loss: 0.680 - ETA: 1s - loss: 0.678 - ETA: 0s - loss: 0.684 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.685 - 6s 800us/step - loss: 0.6872\n",
      "Epoch 59/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.719 - ETA: 5s - loss: 0.703 - ETA: 4s - loss: 0.694 - ETA: 4s - loss: 0.694 - ETA: 3s - loss: 0.688 - ETA: 3s - loss: 0.688 - ETA: 3s - loss: 0.684 - ETA: 2s - loss: 0.686 - ETA: 2s - loss: 0.686 - ETA: 1s - loss: 0.686 - ETA: 1s - loss: 0.682 - ETA: 0s - loss: 0.685 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.687 - 6s 808us/step - loss: 0.6866\n",
      "Epoch 60/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.700 - ETA: 4s - loss: 0.675 - ETA: 4s - loss: 0.674 - ETA: 4s - loss: 0.701 - ETA: 3s - loss: 0.686 - ETA: 3s - loss: 0.689 - ETA: 2s - loss: 0.686 - ETA: 2s - loss: 0.685 - ETA: 2s - loss: 0.682 - ETA: 1s - loss: 0.683 - ETA: 1s - loss: 0.680 - ETA: 0s - loss: 0.678 - ETA: 0s - loss: 0.678 - ETA: 0s - loss: 0.676 - 6s 791us/step - loss: 0.6760\n",
      "Epoch 61/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.647 - ETA: 4s - loss: 0.650 - ETA: 4s - loss: 0.683 - ETA: 4s - loss: 0.668 - ETA: 3s - loss: 0.670 - ETA: 3s - loss: 0.676 - ETA: 2s - loss: 0.673 - ETA: 2s - loss: 0.672 - ETA: 2s - loss: 0.673 - ETA: 1s - loss: 0.677 - ETA: 1s - loss: 0.675 - ETA: 0s - loss: 0.675 - ETA: 0s - loss: 0.677 - ETA: 0s - loss: 0.676 - 6s 790us/step - loss: 0.6778\n",
      "Epoch 62/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.638 - ETA: 4s - loss: 0.673 - ETA: 4s - loss: 0.685 - ETA: 4s - loss: 0.676 - ETA: 3s - loss: 0.669 - ETA: 3s - loss: 0.670 - ETA: 2s - loss: 0.675 - ETA: 2s - loss: 0.681 - ETA: 2s - loss: 0.679 - ETA: 1s - loss: 0.681 - ETA: 1s - loss: 0.679 - ETA: 0s - loss: 0.678 - ETA: 0s - loss: 0.678 - ETA: 0s - loss: 0.680 - 6s 787us/step - loss: 0.6808\n",
      "Epoch 63/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.671 - ETA: 4s - loss: 0.672 - ETA: 4s - loss: 0.666 - ETA: 4s - loss: 0.665 - ETA: 3s - loss: 0.656 - ETA: 3s - loss: 0.655 - ETA: 2s - loss: 0.660 - ETA: 2s - loss: 0.664 - ETA: 2s - loss: 0.664 - ETA: 1s - loss: 0.664 - ETA: 1s - loss: 0.667 - ETA: 0s - loss: 0.669 - ETA: 0s - loss: 0.673 - ETA: 0s - loss: 0.674 - 6s 809us/step - loss: 0.6746\n",
      "Epoch 64/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.656 - ETA: 5s - loss: 0.663 - ETA: 4s - loss: 0.666 - ETA: 4s - loss: 0.664 - ETA: 3s - loss: 0.669 - ETA: 3s - loss: 0.671 - ETA: 2s - loss: 0.672 - ETA: 2s - loss: 0.670 - ETA: 2s - loss: 0.669 - ETA: 1s - loss: 0.668 - ETA: 1s - loss: 0.666 - ETA: 0s - loss: 0.663 - ETA: 0s - loss: 0.665 - ETA: 0s - loss: 0.666 - 6s 818us/step - loss: 0.6662\n",
      "Epoch 65/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.648 - ETA: 4s - loss: 0.678 - ETA: 4s - loss: 0.700 - ETA: 4s - loss: 0.688 - ETA: 3s - loss: 0.689 - ETA: 3s - loss: 0.676 - ETA: 2s - loss: 0.673 - ETA: 2s - loss: 0.673 - ETA: 2s - loss: 0.671 - ETA: 1s - loss: 0.675 - ETA: 1s - loss: 0.671 - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.675 - ETA: 0s - loss: 0.676 - 6s 797us/step - loss: 0.6754\n",
      "Epoch 66/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.652 - ETA: 5s - loss: 0.671 - ETA: 4s - loss: 0.678 - ETA: 4s - loss: 0.673 - ETA: 3s - loss: 0.683 - ETA: 3s - loss: 0.687 - ETA: 2s - loss: 0.682 - ETA: 2s - loss: 0.680 - ETA: 2s - loss: 0.676 - ETA: 1s - loss: 0.671 - ETA: 1s - loss: 0.670 - ETA: 0s - loss: 0.672 - ETA: 0s - loss: 0.668 - ETA: 0s - loss: 0.669 - 6s 808us/step - loss: 0.6686\n",
      "Epoch 67/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.681 - ETA: 4s - loss: 0.674 - ETA: 4s - loss: 0.675 - ETA: 4s - loss: 0.672 - ETA: 3s - loss: 0.667 - ETA: 3s - loss: 0.668 - ETA: 2s - loss: 0.660 - ETA: 2s - loss: 0.661 - ETA: 2s - loss: 0.658 - ETA: 1s - loss: 0.661 - ETA: 1s - loss: 0.664 - ETA: 0s - loss: 0.667 - ETA: 0s - loss: 0.674 - ETA: 0s - loss: 0.672 - 6s 806us/step - loss: 0.6712\n",
      "Epoch 68/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.645 - ETA: 5s - loss: 0.648 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.655 - ETA: 3s - loss: 0.659 - ETA: 3s - loss: 0.661 - ETA: 2s - loss: 0.663 - ETA: 2s - loss: 0.661 - ETA: 2s - loss: 0.658 - ETA: 1s - loss: 0.663 - ETA: 1s - loss: 0.665 - ETA: 0s - loss: 0.668 - ETA: 0s - loss: 0.666 - ETA: 0s - loss: 0.666 - 6s 802us/step - loss: 0.6653\n",
      "Epoch 69/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.678 - ETA: 4s - loss: 0.680 - ETA: 4s - loss: 0.684 - ETA: 4s - loss: 0.687 - ETA: 3s - loss: 0.679 - ETA: 3s - loss: 0.675 - ETA: 2s - loss: 0.669 - ETA: 2s - loss: 0.669 - ETA: 2s - loss: 0.668 - ETA: 1s - loss: 0.663 - ETA: 1s - loss: 0.666 - ETA: 0s - loss: 0.662 - ETA: 0s - loss: 0.664 - ETA: 0s - loss: 0.663 - 6s 788us/step - loss: 0.6639\n",
      "Epoch 70/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.670 - ETA: 5s - loss: 0.626 - ETA: 4s - loss: 0.630 - ETA: 4s - loss: 0.646 - ETA: 3s - loss: 0.650 - ETA: 3s - loss: 0.651 - ETA: 3s - loss: 0.654 - ETA: 2s - loss: 0.650 - ETA: 2s - loss: 0.655 - ETA: 1s - loss: 0.657 - ETA: 1s - loss: 0.656 - ETA: 0s - loss: 0.666 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.664 - 6s 806us/step - loss: 0.6627\n",
      "Epoch 71/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.629 - ETA: 4s - loss: 0.644 - ETA: 4s - loss: 0.657 - ETA: 3s - loss: 0.655 - ETA: 3s - loss: 0.655 - ETA: 3s - loss: 0.657 - ETA: 2s - loss: 0.660 - ETA: 2s - loss: 0.653 - ETA: 2s - loss: 0.654 - ETA: 1s - loss: 0.655 - ETA: 1s - loss: 0.652 - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.651 - 6s 769us/step - loss: 0.6512\n",
      "Epoch 72/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.662 - ETA: 4s - loss: 0.666 - ETA: 4s - loss: 0.670 - ETA: 4s - loss: 0.662 - ETA: 3s - loss: 0.659 - ETA: 3s - loss: 0.652 - ETA: 2s - loss: 0.663 - ETA: 2s - loss: 0.667 - ETA: 2s - loss: 0.666 - ETA: 1s - loss: 0.663 - ETA: 1s - loss: 0.661 - ETA: 0s - loss: 0.658 - ETA: 0s - loss: 0.655 - ETA: 0s - loss: 0.654 - 6s 789us/step - loss: 0.6541\n",
      "Epoch 73/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.638 - ETA: 4s - loss: 0.645 - ETA: 4s - loss: 0.643 - ETA: 4s - loss: 0.647 - ETA: 3s - loss: 0.642 - ETA: 3s - loss: 0.652 - ETA: 2s - loss: 0.652 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.653 - ETA: 1s - loss: 0.652 - ETA: 1s - loss: 0.651 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.650 - 6s 774us/step - loss: 0.6511\n",
      "Epoch 74/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.592 - ETA: 4s - loss: 0.616 - ETA: 4s - loss: 0.629 - ETA: 3s - loss: 0.642 - ETA: 3s - loss: 0.650 - ETA: 3s - loss: 0.654 - ETA: 2s - loss: 0.642 - ETA: 2s - loss: 0.644 - ETA: 2s - loss: 0.643 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.651 - ETA: 0s - loss: 0.652 - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.652 - 6s 778us/step - loss: 0.6515\n",
      "Epoch 75/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.622 - ETA: 5s - loss: 0.634 - ETA: 4s - loss: 0.633 - ETA: 4s - loss: 0.647 - ETA: 3s - loss: 0.642 - ETA: 3s - loss: 0.645 - ETA: 2s - loss: 0.644 - ETA: 2s - loss: 0.644 - ETA: 2s - loss: 0.646 - ETA: 1s - loss: 0.647 - ETA: 1s - loss: 0.643 - ETA: 0s - loss: 0.643 - ETA: 0s - loss: 0.644 - ETA: 0s - loss: 0.649 - 6s 849us/step - loss: 0.6483\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7265/7265 [==============================] - ETA: 5s - loss: 0.618 - ETA: 5s - loss: 0.642 - ETA: 4s - loss: 0.652 - ETA: 4s - loss: 0.655 - ETA: 3s - loss: 0.655 - ETA: 3s - loss: 0.658 - ETA: 3s - loss: 0.655 - ETA: 2s - loss: 0.664 - ETA: 2s - loss: 0.657 - ETA: 1s - loss: 0.658 - ETA: 1s - loss: 0.657 - ETA: 0s - loss: 0.652 - ETA: 0s - loss: 0.653 - ETA: 0s - loss: 0.654 - 6s 832us/step - loss: 0.6543\n",
      "Epoch 77/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.655 - ETA: 5s - loss: 0.652 - ETA: 4s - loss: 0.651 - ETA: 4s - loss: 0.662 - ETA: 3s - loss: 0.666 - ETA: 3s - loss: 0.661 - ETA: 2s - loss: 0.654 - ETA: 2s - loss: 0.650 - ETA: 2s - loss: 0.648 - ETA: 1s - loss: 0.650 - ETA: 1s - loss: 0.651 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.649 - 6s 818us/step - loss: 0.6488\n",
      "Epoch 78/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.619 - ETA: 4s - loss: 0.614 - ETA: 4s - loss: 0.615 - ETA: 4s - loss: 0.624 - ETA: 3s - loss: 0.618 - ETA: 3s - loss: 0.620 - ETA: 2s - loss: 0.626 - ETA: 2s - loss: 0.632 - ETA: 2s - loss: 0.630 - ETA: 1s - loss: 0.635 - ETA: 1s - loss: 0.636 - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.640 - 6s 792us/step - loss: 0.6395\n",
      "Epoch 79/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.661 - ETA: 5s - loss: 0.653 - ETA: 4s - loss: 0.654 - ETA: 4s - loss: 0.648 - ETA: 3s - loss: 0.639 - ETA: 3s - loss: 0.650 - ETA: 2s - loss: 0.646 - ETA: 2s - loss: 0.646 - ETA: 2s - loss: 0.646 - ETA: 1s - loss: 0.637 - ETA: 1s - loss: 0.636 - ETA: 0s - loss: 0.638 - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.634 - 6s 820us/step - loss: 0.6345\n",
      "Epoch 80/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.641 - ETA: 4s - loss: 0.675 - ETA: 4s - loss: 0.668 - ETA: 4s - loss: 0.646 - ETA: 3s - loss: 0.644 - ETA: 3s - loss: 0.637 - ETA: 2s - loss: 0.639 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.645 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.648 - ETA: 0s - loss: 0.652 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.647 - 6s 808us/step - loss: 0.6493\n",
      "Epoch 81/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.618 - ETA: 4s - loss: 0.658 - ETA: 4s - loss: 0.658 - ETA: 4s - loss: 0.656 - ETA: 3s - loss: 0.646 - ETA: 3s - loss: 0.641 - ETA: 2s - loss: 0.632 - ETA: 2s - loss: 0.630 - ETA: 2s - loss: 0.631 - ETA: 1s - loss: 0.629 - ETA: 1s - loss: 0.628 - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.632 - 6s 806us/step - loss: 0.6343\n",
      "Epoch 82/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.661 - ETA: 5s - loss: 0.665 - ETA: 4s - loss: 0.659 - ETA: 4s - loss: 0.648 - ETA: 3s - loss: 0.640 - ETA: 3s - loss: 0.635 - ETA: 2s - loss: 0.630 - ETA: 2s - loss: 0.630 - ETA: 2s - loss: 0.630 - ETA: 1s - loss: 0.630 - ETA: 1s - loss: 0.633 - ETA: 0s - loss: 0.635 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.633 - 6s 805us/step - loss: 0.6334\n",
      "Epoch 83/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.637 - ETA: 4s - loss: 0.634 - ETA: 4s - loss: 0.633 - ETA: 4s - loss: 0.638 - ETA: 3s - loss: 0.638 - ETA: 3s - loss: 0.637 - ETA: 2s - loss: 0.635 - ETA: 2s - loss: 0.631 - ETA: 2s - loss: 0.632 - ETA: 1s - loss: 0.632 - ETA: 1s - loss: 0.632 - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.634 - 6s 819us/step - loss: 0.6360\n",
      "Epoch 84/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.648 - ETA: 4s - loss: 0.644 - ETA: 4s - loss: 0.639 - ETA: 3s - loss: 0.647 - ETA: 3s - loss: 0.645 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.639 - ETA: 2s - loss: 0.641 - ETA: 1s - loss: 0.639 - ETA: 1s - loss: 0.635 - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.633 - 6s 815us/step - loss: 0.6347\n",
      "Epoch 85/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.577 - ETA: 4s - loss: 0.609 - ETA: 4s - loss: 0.624 - ETA: 4s - loss: 0.610 - ETA: 3s - loss: 0.607 - ETA: 3s - loss: 0.607 - ETA: 3s - loss: 0.610 - ETA: 2s - loss: 0.612 - ETA: 2s - loss: 0.620 - ETA: 1s - loss: 0.628 - ETA: 1s - loss: 0.635 - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.636 - 6s 816us/step - loss: 0.6364\n",
      "Epoch 86/100\n",
      "7265/7265 [==============================] - ETA: 6s - loss: 0.641 - ETA: 5s - loss: 0.641 - ETA: 4s - loss: 0.635 - ETA: 4s - loss: 0.631 - ETA: 3s - loss: 0.629 - ETA: 3s - loss: 0.628 - ETA: 3s - loss: 0.627 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.629 - ETA: 1s - loss: 0.630 - ETA: 1s - loss: 0.630 - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.631 - ETA: 0s - loss: 0.633 - 6s 814us/step - loss: 0.6326\n",
      "Epoch 87/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.603 - ETA: 4s - loss: 0.619 - ETA: 4s - loss: 0.611 - ETA: 4s - loss: 0.605 - ETA: 3s - loss: 0.608 - ETA: 3s - loss: 0.607 - ETA: 2s - loss: 0.602 - ETA: 2s - loss: 0.607 - ETA: 2s - loss: 0.608 - ETA: 1s - loss: 0.612 - ETA: 1s - loss: 0.617 - ETA: 0s - loss: 0.619 - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.626 - 6s 822us/step - loss: 0.6273\n",
      "Epoch 88/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.625 - ETA: 5s - loss: 0.654 - ETA: 4s - loss: 0.647 - ETA: 4s - loss: 0.643 - ETA: 3s - loss: 0.641 - ETA: 3s - loss: 0.633 - ETA: 3s - loss: 0.632 - ETA: 2s - loss: 0.628 - ETA: 2s - loss: 0.630 - ETA: 1s - loss: 0.628 - ETA: 1s - loss: 0.630 - ETA: 0s - loss: 0.629 - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.625 - 6s 825us/step - loss: 0.6253\n",
      "Epoch 89/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.579 - ETA: 4s - loss: 0.594 - ETA: 4s - loss: 0.593 - ETA: 4s - loss: 0.604 - ETA: 3s - loss: 0.604 - ETA: 3s - loss: 0.602 - ETA: 2s - loss: 0.608 - ETA: 2s - loss: 0.607 - ETA: 2s - loss: 0.604 - ETA: 1s - loss: 0.605 - ETA: 1s - loss: 0.604 - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.609 - 6s 805us/step - loss: 0.6103\n",
      "Epoch 90/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.591 - ETA: 5s - loss: 0.617 - ETA: 4s - loss: 0.610 - ETA: 4s - loss: 0.617 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.623 - ETA: 2s - loss: 0.626 - ETA: 2s - loss: 0.632 - ETA: 2s - loss: 0.634 - ETA: 1s - loss: 0.636 - ETA: 1s - loss: 0.636 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.630 - 6s 820us/step - loss: 0.6292\n",
      "Epoch 91/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.629 - ETA: 4s - loss: 0.632 - ETA: 4s - loss: 0.639 - ETA: 4s - loss: 0.638 - ETA: 3s - loss: 0.636 - ETA: 3s - loss: 0.629 - ETA: 2s - loss: 0.626 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.619 - ETA: 1s - loss: 0.621 - ETA: 1s - loss: 0.621 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.622 - ETA: 0s - loss: 0.623 - 6s 822us/step - loss: 0.6226\n",
      "Epoch 92/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.645 - ETA: 4s - loss: 0.635 - ETA: 4s - loss: 0.616 - ETA: 4s - loss: 0.602 - ETA: 3s - loss: 0.611 - ETA: 3s - loss: 0.608 - ETA: 2s - loss: 0.614 - ETA: 2s - loss: 0.612 - ETA: 2s - loss: 0.611 - ETA: 1s - loss: 0.612 - ETA: 1s - loss: 0.613 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.609 - ETA: 0s - loss: 0.607 - 6s 781us/step - loss: 0.6085\n",
      "Epoch 93/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.611 - ETA: 5s - loss: 0.620 - ETA: 4s - loss: 0.620 - ETA: 4s - loss: 0.633 - ETA: 3s - loss: 0.626 - ETA: 3s - loss: 0.625 - ETA: 2s - loss: 0.622 - ETA: 2s - loss: 0.621 - ETA: 2s - loss: 0.624 - ETA: 1s - loss: 0.621 - ETA: 1s - loss: 0.616 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.615 - 6s 794us/step - loss: 0.6152\n",
      "Epoch 94/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.645 - ETA: 4s - loss: 0.606 - ETA: 4s - loss: 0.611 - ETA: 4s - loss: 0.617 - ETA: 3s - loss: 0.607 - ETA: 3s - loss: 0.616 - ETA: 2s - loss: 0.609 - ETA: 2s - loss: 0.603 - ETA: 2s - loss: 0.605 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.602 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.603 - 6s 781us/step - loss: 0.6073\n",
      "Epoch 95/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.611 - ETA: 5s - loss: 0.606 - ETA: 4s - loss: 0.615 - ETA: 4s - loss: 0.616 - ETA: 3s - loss: 0.613 - ETA: 3s - loss: 0.615 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.617 - ETA: 1s - loss: 0.616 - ETA: 1s - loss: 0.615 - ETA: 0s - loss: 0.617 - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.620 - 6s 791us/step - loss: 0.6212\n",
      "Epoch 96/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.616 - ETA: 4s - loss: 0.614 - ETA: 4s - loss: 0.621 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.623 - ETA: 2s - loss: 0.622 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.619 - ETA: 1s - loss: 0.619 - ETA: 1s - loss: 0.616 - ETA: 0s - loss: 0.616 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.613 - 6s 776us/step - loss: 0.6125\n",
      "Epoch 97/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.579 - ETA: 4s - loss: 0.596 - ETA: 4s - loss: 0.607 - ETA: 3s - loss: 0.613 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.612 - ETA: 2s - loss: 0.606 - ETA: 2s - loss: 0.606 - ETA: 2s - loss: 0.609 - ETA: 1s - loss: 0.609 - ETA: 1s - loss: 0.608 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.610 - ETA: 0s - loss: 0.612 - 6s 791us/step - loss: 0.6109\n",
      "Epoch 98/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.593 - ETA: 4s - loss: 0.584 - ETA: 4s - loss: 0.589 - ETA: 4s - loss: 0.599 - ETA: 3s - loss: 0.588 - ETA: 3s - loss: 0.585 - ETA: 2s - loss: 0.582 - ETA: 2s - loss: 0.588 - ETA: 2s - loss: 0.588 - ETA: 1s - loss: 0.589 - ETA: 1s - loss: 0.597 - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.602 - ETA: 0s - loss: 0.606 - 6s 784us/step - loss: 0.6049\n",
      "Epoch 99/100\n",
      "7265/7265 [==============================] - ETA: 4s - loss: 0.613 - ETA: 4s - loss: 0.593 - ETA: 4s - loss: 0.602 - ETA: 4s - loss: 0.609 - ETA: 3s - loss: 0.607 - ETA: 3s - loss: 0.606 - ETA: 2s - loss: 0.609 - ETA: 2s - loss: 0.613 - ETA: 2s - loss: 0.613 - ETA: 1s - loss: 0.610 - ETA: 1s - loss: 0.617 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.606 - 6s 789us/step - loss: 0.6061\n",
      "Epoch 100/100\n",
      "7265/7265 [==============================] - ETA: 5s - loss: 0.573 - ETA: 5s - loss: 0.588 - ETA: 4s - loss: 0.596 - ETA: 4s - loss: 0.611 - ETA: 3s - loss: 0.604 - ETA: 3s - loss: 0.604 - ETA: 2s - loss: 0.588 - ETA: 2s - loss: 0.591 - ETA: 2s - loss: 0.593 - ETA: 1s - loss: 0.594 - ETA: 1s - loss: 0.598 - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.600 - 6s 785us/step - loss: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27b8650bc88>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7356, 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_proba(xtest_pad)\n",
    "preds=predictions\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('0000000002.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于前面训练的Word2vec词向量，构建1个2层的GRU模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7265/7265 [==============================] - ETA: 19s - loss: 43655880.000 - ETA: 15s - loss: 45860890.000 - ETA: 14s - loss: 52743210.666 - ETA: 13s - loss: 52235624.000 - ETA: 11s - loss: 50852563.200 - ETA: 10s - loss: 47673459.000 - ETA: 9s - loss: 45607707.142 - ETA: 8s - loss: 42679960.00 - ETA: 6s - loss: 40113673.11 - ETA: 5s - loss: 37131726.10 - ETA: 4s - loss: 34855042.36 - ETA: 2s - loss: 32665632.50 - ETA: 1s - loss: 30706596.30 - ETA: 0s - loss: 28810627.21 - 20s 3ms/step - loss: 28485104.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "7265/7265 [==============================] - ETA: 18s - loss: 3698575.75 - ETA: 18s - loss: 3792359.87 - ETA: 16s - loss: 4049522.58 - ETA: 15s - loss: 4079587.68 - ETA: 13s - loss: 3705752.55 - ETA: 12s - loss: 3401453.06 - ETA: 10s - loss: 3581701.48 - ETA: 9s - loss: 3295170.9375 - ETA: 7s - loss: 3073776.736 - ETA: 6s - loss: 2903420.362 - ETA: 4s - loss: 2746676.431 - ETA: 3s - loss: 2757638.145 - ETA: 1s - loss: 2673727.211 - ETA: 0s - loss: 2679010.571 - 22s 3ms/step - loss: 2696532.2586\n",
      "Epoch 3/100\n",
      "7265/7265 [==============================] - ETA: 20s - loss: 432360.187 - ETA: 19s - loss: 665498.406 - ETA: 17s - loss: 679408.166 - ETA: 15s - loss: 844927.750 - ETA: 14s - loss: 802354.750 - ETA: 12s - loss: 849816.604 - ETA: 11s - loss: 896366.428 - ETA: 9s - loss: 845618.867 - ETA: 8s - loss: 814542.53 - ETA: 6s - loss: 778539.36 - ETA: 4s - loss: 756051.59 - ETA: 3s - loss: 770522.64 - ETA: 1s - loss: 815553.74 - ETA: 0s - loss: 800395.92 - 22s 3ms/step - loss: 848748.8941\n",
      "Epoch 4/100\n",
      "7265/7265 [==============================] - ETA: 20s - loss: 2965272.75 - ETA: 19s - loss: 2460216.62 - ETA: 17s - loss: 1900105.08 - ETA: 16s - loss: 1492209.35 - ETA: 14s - loss: 1298668.98 - ETA: 12s - loss: 1515552.69 - ETA: 11s - loss: 1352930.69 - ETA: 9s - loss: 1295975.5781 - ETA: 8s - loss: 1178887.322 - ETA: 6s - loss: 1136344.065 - ETA: 5s - loss: 1090085.161 - ETA: 3s - loss: 1183205.752 - ETA: 1s - loss: 1201619.790 - ETA: 0s - loss: 1145438.689 - 23s 3ms/step - loss: 1135297.3414\n",
      "Epoch 5/100\n",
      "7265/7265 [==============================] - ETA: 20s - loss: 663598.250 - ETA: 19s - loss: 473498.859 - ETA: 17s - loss: 411540.781 - ETA: 16s - loss: 406591.507 - ETA: 14s - loss: 384495.150 - ETA: 13s - loss: 419368.895 - ETA: 11s - loss: 395240.997 - ETA: 10s - loss: 377965.568 - ETA: 8s - loss: 401310.324 - ETA: 6s - loss: 385998.38 - ETA: 5s - loss: 364887.42 - ETA: 3s - loss: 386407.83 - ETA: 1s - loss: 405011.62 - ETA: 0s - loss: 387975.21 - 23s 3ms/step - loss: 383549.3961\n",
      "Epoch 6/100\n",
      "7265/7265 [==============================] - ETA: 20s - loss: 1037582.06 - ETA: 19s - loss: 1621770.78 - ETA: 17s - loss: 1277328.04 - ETA: 16s - loss: 1422757.28 - ETA: 14s - loss: 1243508.85 - ETA: 13s - loss: 1169064.33 - ETA: 11s - loss: 1097469.94 - ETA: 9s - loss: 994782.2734 - ETA: 8s - loss: 949131.32 - ETA: 6s - loss: 859000.25 - ETA: 5s - loss: 815722.05 - ETA: 3s - loss: 783768.91 - ETA: 1s - loss: 783530.55 - ETA: 0s - loss: 866655.97 - 23s 3ms/step - loss: 856347.6770\n",
      "Epoch 7/100\n",
      "7265/7265 [==============================] - ETA: 21s - loss: 368926.281 - ETA: 20s - loss: 310185.562 - ETA: 18s - loss: 1035178.04 - ETA: 16s - loss: 853172.2500 - ETA: 15s - loss: 890032.887 - ETA: 13s - loss: 749003.591 - ETA: 11s - loss: 682275.868 - ETA: 10s - loss: 882710.634 - ETA: 8s - loss: 834836.105 - ETA: 6s - loss: 764331.09 - ETA: 5s - loss: 716839.24 - ETA: 3s - loss: 698871.73 - ETA: 1s - loss: 658317.43 - ETA: 0s - loss: 727341.99 - 23s 3ms/step - loss: 731129.7266\n",
      "Epoch 8/100\n",
      "7265/7265 [==============================] - ETA: 21s - loss: 237807.796 - ETA: 19s - loss: 487196.523 - ETA: 18s - loss: 412706.963 - ETA: 16s - loss: 698326.160 - ETA: 15s - loss: 724280.215 - ETA: 13s - loss: 644809.859 - ETA: 11s - loss: 661158.433 - ETA: 10s - loss: 586193.648 - ETA: 8s - loss: 534720.875 - ETA: 6s - loss: 514749.01 - ETA: 5s - loss: 483024.61 - ETA: 3s - loss: 536183.50 - ETA: 2s - loss: 648216.78 - ETA: 0s - loss: 620224.90 - 24s 3ms/step - loss: 613389.3699\n",
      "Epoch 9/100\n",
      "7265/7265 [==============================] - ETA: 22s - loss: 266453.218 - ETA: 20s - loss: 539180.734 - ETA: 18s - loss: 428251.338 - ETA: 17s - loss: 334802.145 - ETA: 15s - loss: 520320.316 - ETA: 13s - loss: 538743.805 - ETA: 12s - loss: 751679.904 - ETA: 10s - loss: 685776.238 - ETA: 8s - loss: 631592.585 - ETA: 7s - loss: 603221.31 - ETA: 5s - loss: 562232.00 - ETA: 3s - loss: 548369.15 - ETA: 2s - loss: 524452.92 - ETA: 0s - loss: 494733.87 - 24s 3ms/step - loss: 491131.1139\n",
      "Epoch 10/100\n",
      "7265/7265 [==============================] - ETA: 21s - loss: 316379.812 - ETA: 20s - loss: 204602.039 - ETA: 18s - loss: 261364.796 - ETA: 16s - loss: 472080.222 - ETA: 15s - loss: 808703.678 - ETA: 13s - loss: 696758.796 - ETA: 12s - loss: 900920.433 - ETA: 10s - loss: 802546.380 - ETA: 8s - loss: 748140.536 - ETA: 7s - loss: 699567.10 - ETA: 5s - loss: 679944.18 - ETA: 3s - loss: 628265.26 - ETA: 2s - loss: 597064.14 - ETA: 0s - loss: 579282.65 - 24s 3ms/step - loss: 574376.0787\n",
      "Epoch 11/100\n",
      "7265/7265 [==============================] - ETA: 21s - loss: 90140.73 - ETA: 20s - loss: 121199.726 - ETA: 18s - loss: 298685.109 - ETA: 16s - loss: 378776.003 - ETA: 15s - loss: 371356.796 - ETA: 13s - loss: 639030.059 - ETA: 11s - loss: 553426.434 - ETA: 10s - loss: 741125.645 - ETA: 8s - loss: 689713.532 - ETA: 7s - loss: 642648.49 - ETA: 5s - loss: 627830.69 - ETA: 3s - loss: 587545.45 - ETA: 2s - loss: 567646.57 - ETA: 0s - loss: 546748.54 - 24s 3ms/step - loss: 541291.7826\n",
      "Epoch 12/100\n",
      "7265/7265 [==============================] - ETA: 22s - loss: 150586.812 - ETA: 20s - loss: 160808.539 - ETA: 18s - loss: 311164.526 - ETA: 17s - loss: 436845.660 - ETA: 15s - loss: 377882.250 - ETA: 13s - loss: 326943.505 - ETA: 12s - loss: 302302.700 - ETA: 10s - loss: 273526.176 - ETA: 8s - loss: 258693.992 - ETA: 7s - loss: 254629.58 - ETA: 5s - loss: 279968.34 - ETA: 3s - loss: 269978.91 - ETA: 2s - loss: 268331.71 - ETA: 0s - loss: 281322.50 - 24s 3ms/step - loss: 280215.1793\n",
      "Epoch 13/100\n",
      "1024/7265 [===>..........................] - ETA: 22s - loss: 100315.593 - ETA: 22s - loss: 62077.7090 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-96f3e2a538d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=100, \n\u001b[1;32m----> 3\u001b[1;33m           verbose=1, callbacks=[earlystop])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(xtest_pad)\n",
    "preds=predictions\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('0000000003.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7265/7265 [==============================] - ETA: 30s - loss: 1.08 - ETA: 24s - loss: 1.01 - ETA: 21s - loss: 1.01 - ETA: 18s - loss: 0.98 - ETA: 16s - loss: 0.98 - ETA: 14s - loss: 0.97 - ETA: 13s - loss: 0.96 - ETA: 11s - loss: 0.96 - ETA: 9s - loss: 0.9657 - ETA: 7s - loss: 0.958 - ETA: 5s - loss: 0.955 - ETA: 3s - loss: 0.955 - ETA: 2s - loss: 0.951 - ETA: 0s - loss: 0.950 - 26s 4ms/step - loss: 0.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "7265/7265 [==============================] - ETA: 24s - loss: 0.90 - ETA: 22s - loss: 0.89 - ETA: 20s - loss: 0.90 - ETA: 19s - loss: 0.89 - ETA: 17s - loss: 0.88 - ETA: 15s - loss: 0.87 - ETA: 13s - loss: 0.87 - ETA: 11s - loss: 0.88 - ETA: 9s - loss: 0.8840 - ETA: 8s - loss: 0.881 - ETA: 6s - loss: 0.880 - ETA: 4s - loss: 0.877 - ETA: 2s - loss: 0.872 - ETA: 0s - loss: 0.870 - 28s 4ms/step - loss: 0.8700\n",
      "Epoch 3/100\n",
      "6656/7265 [==========================>...] - ETA: 26s - loss: 0.77 - ETA: 24s - loss: 0.78 - ETA: 22s - loss: 0.79 - ETA: 20s - loss: 0.82 - ETA: 18s - loss: 0.82 - ETA: 16s - loss: 0.82 - ETA: 14s - loss: 0.81 - ETA: 12s - loss: 0.81 - ETA: 10s - loss: 0.81 - ETA: 8s - loss: 0.8167 - ETA: 6s - loss: 0.816 - ETA: 4s - loss: 0.817 - ETA: 2s - loss: 0.8196"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-96f3e2a538d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=100, \n\u001b[1;32m----> 3\u001b[1;33m           verbose=1, callbacks=[earlystop])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_total_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(xtest_pad)\n",
    "preds=predictions\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('0000000004.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
