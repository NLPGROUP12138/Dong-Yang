{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import time\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据整合，输出新数据\n",
    "def merge_file(file1,file2):\n",
    "    feature = pd.read_csv(file1,sep=\",\")\n",
    "    label = pd.read_csv(file2,sep=\",\")\n",
    "    data = feature.merge(label,on='id') # 按照id列整合两个训练集表\n",
    "    data[\"X\"] = data[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1) # \"title\",\"content\"两列合为一列\"X\"\n",
    "    df = data[['id', 'title', 'content', 'label', 'X']]\n",
    "    return df\n",
    "\n",
    "\n",
    "data = merge_file(\"Train_DataSet.csv\",\"Train_DataSet_Label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 停用词加载和清洗\n",
    "def stopwords_list(filepath):\n",
    "    stopwords=[line.strip() for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords=stopwords_list('stopwords.txt')\n",
    "\n",
    "def stop_clean(strs):\n",
    "    strs = [w for w in strs if not w in stopwords]\n",
    "    return strs\n",
    "\n",
    "# 只保留汉字\n",
    "def character_save(strs):\n",
    "    strs = re.findall(r'[\\u4e00-\\u9fa5]',strs)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dongy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.613 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#处理训练集，将训练集的文本信息和label信息合并，清洗特殊符合，同时将文本内容进行分词\n",
    "def data_pre_handle(df):\n",
    "    # 1.去空值\n",
    "    dataDropNa=df.dropna(axis=0, how='any')  \n",
    "#    dfDropNa[\"X\"]=dataDropNa[\"X\"].apply(lambda x: str(x).replace(\"\\\\n\",\"\").replace(\".\",\"\").replace(\"\\n\",\"\").replace(\"　\",\"\").replace(\"↓\",\"\").replace(\"/\",\"\").replace(\"|\",\"\").replace(\" \",\"\"))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X\"]\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\"\".join(character_save(x)))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\"\".join(stop_clean(x)))\n",
    "    dataDropNa[\"X_split\"]=dataDropNa[\"X_split\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "    \n",
    "    \n",
    "\n",
    "    return dataDropNa\n",
    " \n",
    "data = data_pre_handle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       问责 领导 上 黄镇 党委书记 张涛 宣国 才 真 一手遮天   天 看  人 举报 施  ...\n",
       "1       江歌 事件 教会 孩子 善良  时 更 懂 保护  去 一年  江歌 悲剧  日 再次 刷 ...\n",
       "2       绝味 鸭 脖 广告 开 黄腔 引 众怒 双  拼值  双 亿  销售额  中国  全世界 感...\n",
       "3       央视 曝光 东 一 医药企业  槽罐车 改成 垃圾车 夜间 偷排 高浓度 废水 年  东 高...\n",
       "4       恶劣 极 央视 都 曝光  南通 东 一 医药企业  槽罐车 改成 洒水车 夜间 偷排 高浓...\n",
       "5       央视 曝光 南通 一 医药企业 夜间 偷排 高浓度 废水 丢脸 昨晚 央视 一套 晚间新闻 ...\n",
       "6       粉丝 爆料 五洲 国际 无锡 项目 涉嫌 诈骗 非法 集资  金融街 号   非法 集资 诈...\n",
       "7       年内 约  锂电 重组 失败 资  高 估值 收购 说 不 摘  中国 电池 联盟  数 显...\n",
       "8       男子 梦想 一夜 暴富 持 水泥块 砸机 一分钱 都 没取  近日 江苏 扬州 谢  盗取 ...\n",
       "9       北京 家 法院 供暖 纠纷案件 主体  供暖费 追缴 山海 网 北京   冬天   年 更 ...\n",
       "10      手机号 开头  注意 看 完   冷汗 都 出  搜 搜狐 原 标题 手机号 开头  注意 ...\n",
       "11      网红 土坯房 书记 落马  背后 原 标题 网红 土坯房 书记 落马  背后 深耕 卢氏 官...\n",
       "12      曾  品质 烤肉 两年    难吃  三鼎 甲 曾  尔滨 烤肉 界 位 显赫 两年 前去 ...\n",
       "13      土俄 军火交易 再 波折 鱼熊 无法 兼顾 土  成美 俄 交锋 牺牲品 搜狐 军事 搜狐网...\n",
       "14        一  闲鱼  网上交易 平台 购买  一 代付款 服务    国外 网站 上 购物   ...\n",
       "15      标题 不 签字 不 意 不 反抗  些 人扮 普天庆  样子 好   新兴 商铺 拆迁  态...\n",
       "16      伙   已 说  教育 废品 文 不 作文 一篇 文章 写  写   不 知道  跑   天...\n",
       "17      车震 视频 外泄 执法  出现 娱乐 化 倾 一度 引发 关注  河北 馆陶 辅警 执法 中...\n",
       "18      全球 警戒 美 官方 称 海外 美国 人  面 恐袭 危险 法治 中国 环球网 综合 报道 ...\n",
       "19      图 信宜市 区 一 男子 疑饮醉 竟濑尿 开 车门 趴 驾驶室 月 日 晚 信宜市 区 南方...\n",
       "20      江歌案  日 开审 真相 便 代价 不必 逢迎 想象 中国 焦点 新闻网 导读 颠末  三 ...\n",
       "21      九江 大桥  举报 空心 桥墩 底  成  机密 专家 沉默 博思网 摘 距 九江 大桥 坍...\n",
       "22      女子 裸贷发 裸 后 没收  钱 反遭 敲诈 还  求 陪 睡 女子 想 裸贷发  裸 后 ...\n",
       "23      马蓉 现身 扮 似 少女  朋友 热聊 儿子  一旁 完全 忽视 令 众人 气愤 导读 原 ...\n",
       "24      忻州 男子 烧 垃圾  罚 元 理 却  奇葩 忻州 男子 烧 垃圾  罚 元 理 却  奇...\n",
       "25      联播 西安 一 饭店 违规 燃放烟花 爆竹 店主  罚款 元 行政拘留 天 陕西 新闻联播 ...\n",
       "26      耶路撒冷 中央 车站 发生 持刀 袭击 事件  时间 号 下午 位 耶路撒冷 市内  中央 ...\n",
       "27      紧急通知 正 看微信   不 轻易 透露 串 数字 马上 告诉 家里人 原 标题 紧急通知 ...\n",
       "28      教科书 式 耍赖 受害  鉴定 车祸  死 家属  追 老赖 刑责 云 备受 关注  教科书...\n",
       "29      袁立手 撕 演员  诞生 节目组 走后门 失败 反帅 锅  小 导演 今天 娱乐圈  上演 ...\n",
       "                              ...                        \n",
       "7308    阴雨  些 危害 安徽 首页 中国 天气 网 中国 天气 网 分钟 前纳特 加强  飓风 已...\n",
       "7310    市中区 王官庄 街道 十 区 社区 发放 消防安全 告知 书 进一步 做好 辖区 火灾 防控...\n",
       "7311    华北 中南部 黄淮 汾渭    霾 南方 区 持续 阴雨 西藏 原 标题 华北 中南部 黄淮...\n",
       "7312    火劫重 生前 女足 球员 流泪 说出  真相 女子 原 标题 火劫重 生前 女足 球员 流泪...\n",
       "7313    开学 一课 观山 湖区 百余 萌娃学 火场 逃生 彩 贵州 网讯 谢静 网 记 杨艳 进一步...\n",
       "7314    看 渭南 渭南 发生  大事 小事 看 里 点击 关注 掌上 渭南 彩 呈现   月 日 阴...\n",
       "7315    中国 百慕大 鄱阳湖 老爷庙 水域 年 吞船 艘  中国 江西省  老爷庙 水域 仅  年代...\n",
       "7316    电力 课堂  线下 违法 建房 亮 红牌 电力设施 保护 条例 十五条  规定  单位  人...\n",
       "7317    台风 少年 团 原  希   村民 一句 话 孩子  失  台风 少年 团 原  希   村...\n",
       "7318       生不出 儿子 豪门 弃妇 最爽 逆袭 老娘 身价 亿原 标题    生不出 儿子 豪门...\n",
       "7319    日 官员 东京 奥运会   人 吃 上 核灾 食品  还 手机 网易 网 列位 欢迎  理说...\n",
       "7321    高伟光 主演  怒 晴 湘西 鹧鸪 哨 背 红 姑娘 爬 上 山崖 奇迹 鹧鸪 哨  红 姑...\n",
       "7322    民间事 老实人  财运 长飞镇   吴 财主 宅子  成片 儿子 坐 一桌 单说  岁 年 ...\n",
       "7323    湖南 水电 职院 召开 赴 沙巴 大学 交流学习 动员会 频道 首页 水利 闻 水利建设 水...\n",
       "7324    平塘 交警 泼  冷水  还 心存 感激  底 回事 老 司机 都 知道 驾驶 大 货车 下...\n",
       "7325    杨紫 爸爸 刺痛 无数 人 父母    死神 间  一堵 墙 点选 设 星标 首席 君 天 ...\n",
       "7326    欲 钱 买 劳怨  动物  什 百度 知道 狗 劳怨 应  主动   不 动  看 报道  ...\n",
       "7327    吉祥 虎 老虎机 最新 平台 吉祥 虎 老虎机 最新 平台 派克   一条条 生命   不 ...\n",
       "7328    广州 学生 遇 暴雨 预警  延迟 上学 不算 迟 旷课 奎屯 新闻网 奎屯 新快报 讯 记...\n",
       "7329    缅甸 赌场 缅甸 赌场 步步 惊心 章 厌烦  只 想 婚 孩儿  回 成绩单 二楼  走廊...\n",
       "7330    帮 女郎  行动 湖北 黄冈 面包车 逆行  卡车 相撞 致一人 困 消防员 成功 救援 月...\n",
       "7331    稻城 亚丁 川藏线 拉萨 青藏线  心动   行程 简介 一阶段  天桥 天 天镇 天 一日...\n",
       "7332    大良 龙江 陈村  学校 月 开学 德 未 两年 力大良 龙江 陈村  学校 月 开学 德 ...\n",
       "7333    最 容易 受 甲醛 侵害  五类 人   污染 原 标题 最 容易 受 甲醛 侵害  五类 ...\n",
       "7334    年 月 事 伤亡 月 报年 月份 全市 生产 安全事 基 情况 年 月份 全市 共 发生 生...\n",
       "7335    珊瑚 裸尾鼠 首  全球 气候 变暖 灭绝  哺乳动物 凯文 登上 前 澳大利亚  飞机 飞...\n",
       "7336    独居 老人 做饭 忘关 火 南通 志愿 时发现 转危安 江海 明珠 网讯 记 修雨竹 汤思敏...\n",
       "7337     生意 上  人  利 合诈骗 诈骗 三十万 够判 少 年  律师 诈骗罪 量刑 标准  加...\n",
       "7338    奎山 汽贸 城 去年 场 火灾 调查 情况 报告 出  日 日 济 技术开发区 发布 关 奎...\n",
       "7339    曝光 台 调查 市场 消防通道  长期 霸占 事情 非 想象  样 消防通道  生命 通道 ...\n",
       "Name: X_split, Length: 7265, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"X_split\"][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #处理训练集，将训练集的文本信息和label信息合并，清洗特殊符合，同时将文本内容进行分词\n",
    "# def merge_feature_label(file1,file2):\n",
    "#     feature=pd.read_csv(file1,sep=\",\")\n",
    "#     label=pd.read_csv(file2,sep=\",\")\n",
    "#     data=feature.merge(label,on='id') # 按照id列整合两个训练集表\n",
    "#     data[\"X\"]=data[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1) # 两列合为一列\n",
    "#     dataDropNa=data.dropna(axis=0, how='any')  # 去空值\n",
    "#     dataDropNa[\"X\"]=dataDropNa[\"X\"].apply(lambda x: str(x).replace(\"\\\\n\",\"\").replace(\".\",\"\").replace(\"\\n\",\"\").replace(\"　\",\"\").replace(\"↓\",\"\").replace(\"/\",\"\").replace(\"|\",\"\").replace(\" \",\"\"))\n",
    "#     dataDropNa[\"X_split\"]=dataDropNa[\"X\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "#     return dataDropNa\n",
    " \n",
    "# dataDropNa=merge_feature_label(\"Train_DataSet.csv\",\"Train_DataSet_Label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def clean_chineses_text(text):\n",
    "#     \"\"\"\n",
    "#     中文数据清洗  stopwords_chineses.txt存放在博客园文件中\n",
    "#     :param text:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     text = BeautifulSoup(text, 'html.parser').get_text() #去掉html标签\n",
    "#     text =jieba.lcut(text);\n",
    "#     stopwords = {}.fromkeys([line.rstrip() for line in open('stopwords.txt')]) #加载停用词(中文)\n",
    "#     eng_stopwords = set(stopwords) #去掉重复的词\n",
    "#     words = [w for w in text if w not in eng_stopwords] #去除文本中的停用词\n",
    "#     return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#处理测试数据\n",
    "def process_test(test_name):\n",
    "    test=pd.read_csv(test_name,sep=\",\")\n",
    "    test[\"X\"]=test[[\"title\",\"content\"]].apply(lambda x:\"\".join([str(x[0]),str(x[1])]),axis=1)\n",
    "    test[\"X_split\"]=test[\"X\"].apply(lambda x:\"\".join(character_save(x)))\n",
    "    test[\"X_split\"]=test[\"X_split\"].apply(lambda x:\" \".join(jieba.cut(x)))\n",
    "    test[\"X_split\"]=test[\"X_split\"].apply(lambda x:\"\".join(stop_clean(x)))\n",
    "    \n",
    "    return test\n",
    " \n",
    "testData=process_test(\"Test_DataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=4, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取文本内容的tf-idf表示\n",
    "xTrain=data[\"X_split\"]\n",
    "xTest=testData[\"X_split\"]\n",
    "# TfidfVectorizer生成词向量\n",
    "vec = TfidfVectorizer(ngram_range=(1,2),min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "# Fit_transform学习到一个字典，并返回Document-term的矩阵(即词典中的词在该文档中出现的频次)\n",
    "xTrain_tfidf = vec.fit_transform(xTrain)  # 先拟合数据，然后转化它将其转化为标准形式，一般应用在训练集\n",
    "xTest_tfidf = vec.transform(xTest)  # 找中心和缩放等实现标准化，一般用在测试集\n",
    "yTrain=data[\"label\"]  # y值\n",
    "\n",
    "#训练逻辑回归模型\n",
    "clf = LogisticRegression(C=4, dual=True)\n",
    "clf.fit(xTrain_tfidf, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#预测测试集，并生成结果提交\n",
    "preds=clf.predict_proba(xTest_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds=np.argmax(preds,axis=1) # 当axis=1时，表示返回行方向上数值最大值下标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred=pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7356, 1)\n"
     ]
    }
   ],
   "source": [
    "test_pred.columns=[\"label\"]\n",
    "print(test_pred.shape)\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('sub_lr_baseline3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练支持向量机模型\n",
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(xTrain_tfidf, yTrain)\n",
    " \n",
    "#预测测试集，并生成结果提交\n",
    "preds=lin_clf.predict(xTest_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred=pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred.columns=[\"label\"]\n",
    "test_pred[\"id\"]=list(testData[\"id\"])\n",
    "test_pred[[\"id\",\"label\"]].to_csv('sub_svm_baseline3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
