{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai with HuggingFace 🤗Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简介:NLP中迁移学习的故事**\n",
    "\n",
    "在2018年初，Jeremy Howard (fast.ai的联合创始人)和Sebastian Ruder介绍了用于文本分类的通用语言模型微调(ULMFiT)方法。ULMFiT是第一个应用于NLP的迁移学习方法。其结果是，除了显著优于许多最先进的任务外，它还允许仅使用100个标记的示例来匹配相当于在100×更多数据上训练的模型的性能。\n",
    "\n",
    "\n",
    "\n",
    "我第一次听说ULMFiT是在一次斋戒期间。这门课是杰里米·霍华德开的。他演示了如何使用几行代码实现完整的ULMFit方法，这要感谢fastai库。在演示中，他使用了在Wikitext-103上预先训练的AWD-LSTM神经网络，并迅速获得了最先进的结果。他还解释了关键的技术——在ULMFiT中也有演示——来微调模型，如区分学习速率、逐步解冻或倾斜三角形学习速率。\n",
    "\n",
    "\n",
    "\n",
    "自从ULMFiT的引入，转移学习在NLP中变得非常流行，然而谷歌(BERT, Transformer-XL, XLNet)、Facebook (RoBERTa, XLM)甚至OpenAI (GPT, GPT-2)开始在非常大的料库上对自己的模型进行预训练。这一次，他们没有使用AWD-LSTM神经网络，而是使用了一个更强大的基于Transformer的架构(cf. Attention is all you need)。\n",
    "\n",
    "\n",
    "\n",
    "尽管这些模型功能强大，但是fastai并没有集成所有的模型。幸运的是,HuggingFace🤗都知道Transformer库创建的。以前被称为pytorch-transformer或pytorch- pretraining - BERT，这个库汇集了40多个最先进的预培训的NLP模型(BERT, GPT-2, RoBERTa, CTRL…)。该实现提供了一些有趣的附加实用程序，如记号赋予器、优化器或调度器。\n",
    "\n",
    "\n",
    "\n",
    "Transformer库可以自给自足，但将其整合到fastai库中，可以提供更简单的实现，并与功能强大的fastai工具兼容，如区分学习速率、逐步解压或倾斜三角形学习速率。这里的要点是允许非NLP专家轻松获得最新的结果，因此“让NLP不再酷”。\n",
    "\n",
    "值得注意的是，HuggingFace transformer库在fastai的集成已经在以下方面进行了演示:\n",
    "\n",
    "Keita Kurita的文章是用fastai微调BERT的教程，使pytorch_pretrained_bert库与fastai兼容。\n",
    "\n",
    "Dev Sharma的文章使用RoBERTa与Fastai在NLP使pytorch_transformers库与Fastai兼容。\n",
    "\n",
    "虽然这些文章都是高质量的，注意他们的演示是不兼容的最后版本的transformer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**集成transformer与fastai多分类**\n",
    "\n",
    "在开始实现之前，请注意，可以通过多种不同的方式在fastai中集成transformer。出于这个原因，我决定带来最通用、最灵活的简单解决方案。更准确地说，我尝试在两个库中进行最小的修改，同时使它们与最大数量的transformer架构兼容。\n",
    "\n",
    "请注意，除了这个笔记本和这篇媒体文章之外，我还在我的GitHub(TODO add link)上发布了另一个版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path \n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e4b766de8315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# transformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
