{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preface\n",
    "\n",
    "这个notebook是为了工作面试的家庭作业而设计的。这个问题是一个经典的多类分类(不是积极/消极的预测，而是逐渐从0到4的情绪量表)。最直接的方式解决这是使用一些分类algorythm,像k - means,不过我最近做了蕴藏在深度学习和情感分析的主题(Udacity深度学习Nanodegree奖学金挑战,课上情绪分析RNN),所以我决定尝试和应用方法。此外，循环神经网络(RNN)在这类任务中具有较高的准确性。\n",
    "\n",
    "\n",
    "\n",
    "如果需要，我将使用Pytorch库进行模型构建，使用NLTK进行文本预处理。为了节省时间，我也不会尝试重新发明轮子，而是重用来自在线资源和Udacity课程材料的代码示例。\n",
    "\n",
    "\n",
    "\n",
    "从排行榜来看，任何高于0.65分的成绩都可以认为是本次比赛的好成绩，所以我将以此为基准来评判我的model表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "print(os.listdir(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\\train.tsv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\\test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4079.732744</td>\n",
       "      <td>2.063578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>45050.785842</td>\n",
       "      <td>2502.764394</td>\n",
       "      <td>0.893832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39015.750000</td>\n",
       "      <td>1861.750000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4017.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>117045.250000</td>\n",
       "      <td>6244.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>8544.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PhraseId     SentenceId      Sentiment\n",
       "count  156060.000000  156060.000000  156060.000000\n",
       "mean    78030.500000    4079.732744       2.063578\n",
       "std     45050.785842    2502.764394       0.893832\n",
       "min         1.000000       1.000000       0.000000\n",
       "25%     39015.750000    1861.750000       2.000000\n",
       "50%     78030.500000    4017.000000       2.000000\n",
       "75%    117045.250000    6244.000000       3.000000\n",
       "max    156060.000000    8544.000000       4.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>for</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>is also</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>gander</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which occasionally amuses but none of ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>some</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>of which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>occasionally amuses but none of which amounts ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>amuses but none of which amounts to much of a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>amuses</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>but none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>but</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>of which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>amounts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>much</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0          1           1  A series of escapades demonstrating the adage ...   \n",
       "1          2           1  A series of escapades demonstrating the adage ...   \n",
       "2          3           1                                           A series   \n",
       "3          4           1                                                  A   \n",
       "4          5           1                                             series   \n",
       "5          6           1  of escapades demonstrating the adage that what...   \n",
       "6          7           1                                                 of   \n",
       "7          8           1  escapades demonstrating the adage that what is...   \n",
       "8          9           1                                          escapades   \n",
       "9         10           1  demonstrating the adage that what is good for ...   \n",
       "10        11           1                            demonstrating the adage   \n",
       "11        12           1                                      demonstrating   \n",
       "12        13           1                                          the adage   \n",
       "13        14           1                                                the   \n",
       "14        15           1                                              adage   \n",
       "15        16           1                    that what is good for the goose   \n",
       "16        17           1                                               that   \n",
       "17        18           1                         what is good for the goose   \n",
       "18        19           1                                               what   \n",
       "19        20           1                              is good for the goose   \n",
       "20        21           1                                                 is   \n",
       "21        22           1                                 good for the goose   \n",
       "22        23           1                                               good   \n",
       "23        24           1                                      for the goose   \n",
       "24        25           1                                                for   \n",
       "25        26           1                                          the goose   \n",
       "26        27           1                                              goose   \n",
       "27        28           1  is also good for the gander , some of which oc...   \n",
       "28        29           1  is also good for the gander , some of which oc...   \n",
       "29        30           1                                            is also   \n",
       "..       ...         ...                                                ...   \n",
       "33        34           1  the gander , some of which occasionally amuses...   \n",
       "34        35           1                                       the gander ,   \n",
       "35        36           1                                         the gander   \n",
       "36        37           1                                             gander   \n",
       "37        38           1                                                  ,   \n",
       "38        39           1  some of which occasionally amuses but none of ...   \n",
       "39        40           1                                      some of which   \n",
       "40        41           1                                               some   \n",
       "41        42           1                                           of which   \n",
       "42        43           1                                              which   \n",
       "43        44           1  occasionally amuses but none of which amounts ...   \n",
       "44        45           1                                       occasionally   \n",
       "45        46           1  amuses but none of which amounts to much of a ...   \n",
       "46        47           1                                             amuses   \n",
       "47        48           1       but none of which amounts to much of a story   \n",
       "48        49           1                                                but   \n",
       "49        50           1           none of which amounts to much of a story   \n",
       "50        51           1                                               none   \n",
       "51        52           1                of which amounts to much of a story   \n",
       "52        53           1                   which amounts to much of a story   \n",
       "53        54           1                         amounts to much of a story   \n",
       "54        55           1                                            amounts   \n",
       "55        56           1                                 to much of a story   \n",
       "56        57           1                                                 to   \n",
       "57        58           1                                    much of a story   \n",
       "58        59           1                                               much   \n",
       "59        60           1                                         of a story   \n",
       "60        61           1                                            a story   \n",
       "61        62           1                                              story   \n",
       "62        63           1                                                  .   \n",
       "\n",
       "    Sentiment  \n",
       "0           1  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "4           2  \n",
       "5           2  \n",
       "6           2  \n",
       "7           2  \n",
       "8           2  \n",
       "9           2  \n",
       "10          2  \n",
       "11          2  \n",
       "12          2  \n",
       "13          2  \n",
       "14          2  \n",
       "15          2  \n",
       "16          2  \n",
       "17          2  \n",
       "18          2  \n",
       "19          2  \n",
       "20          2  \n",
       "21          3  \n",
       "22          3  \n",
       "23          2  \n",
       "24          2  \n",
       "25          2  \n",
       "26          2  \n",
       "27          2  \n",
       "28          2  \n",
       "29          2  \n",
       "..        ...  \n",
       "33          1  \n",
       "34          2  \n",
       "35          2  \n",
       "36          2  \n",
       "37          2  \n",
       "38          2  \n",
       "39          2  \n",
       "40          2  \n",
       "41          2  \n",
       "42          2  \n",
       "43          2  \n",
       "44          2  \n",
       "45          2  \n",
       "46          3  \n",
       "47          1  \n",
       "48          2  \n",
       "49          1  \n",
       "50          2  \n",
       "51          2  \n",
       "52          2  \n",
       "53          2  \n",
       "54          2  \n",
       "55          2  \n",
       "56          2  \n",
       "57          2  \n",
       "58          2  \n",
       "59          2  \n",
       "60          2  \n",
       "61          2  \n",
       "62          2  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train['SentenceId'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如在最初的竞赛描述中所提到的，没有单独的电影评论，而是断章取义，分成更小的部分，每个部分都有指定的情感类别。比赛是根据每个测试阶段的评分结果进行评估的，因此在这里，整个评审的上下文并不重要。数据也相当干净，因此不需要太多的预处理。在继续之前，最好先看看数据的分布，看看训练集中的类是否均匀分布。为此，我借用了Kaggle另一个内核的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "0     7072\n",
       "1    27273\n",
       "2    79582\n",
       "3    32927\n",
       "4     9206\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立一个词典key为Sentiment值，value为该Sentiment值的个数\n",
    "dist = df_train.groupby([\"Sentiment\"]).size()\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+4AAAK4CAYAAADqRKfgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2473b402da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seaborn高效绘图\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(dist.keys(), dist.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似乎遵循正态分布，最常见的分布是“2”。这可能导致模型没有足够的数据来学习较少表示的类。在评估模型时要注意这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理\n",
    "\n",
    "单词需要标记成数字格式，然后传递给RNN。在此之前，我还将过滤掉空格和标点符号，并使用lemm进一步降低维度。此时，我不想过滤掉“停止词”，因为RNN善于从以前遇到的信息中学习上下文。在电影评论中，短语\"this movie is shit\"与\"this movie is the shit\"有相反的意思，所以我希望模型可以获得这些信息。\n",
    "\n",
    "以下是我从网上借来的一些帮助准备数据的辅助功能:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words 从分词列表中删除非ascii字符\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words 从分词列表中删除标点符号\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words 词干\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words 词原型 \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，遍历dataframe并使用NLTK分词。然后将通过之前创建的预处理函数处理每个分词，最终结果是一个简化的分词列表:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, series, of, escapades, demonstrating, the,...\n",
       "1    [a, series, of, escapades, demonstrating, the,...\n",
       "2                                          [a, series]\n",
       "3                                                  [a]\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step - tokenizing phrases\n",
    "df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Second step - passing through prep functions\n",
    "df_train['Words'] = df_train['Words'].apply(normalize) \n",
    "df_train['Words'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将单词转换为数字表示形式，因为嵌入查找要求将整数传递到网络。最简单的方法是创建字典，将词汇表中的单词映射到整数。使用这个词典，那么每个短语中的单词可以转换成整数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16209\n",
      "16209\n"
     ]
    }
   ],
   "source": [
    "# Third step - creating a list of unique words to be used as dictionary for encoding\n",
    "word_set = set()\n",
    "for l in df_train['Words']:\n",
    "    for e in l:\n",
    "        word_set.add(e)\n",
    "        \n",
    "word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n",
    "\n",
    "# Check if they are still the same lengh\n",
    "print(len(word_set))\n",
    "print(len(word_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim, logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 17:41:20,710 : INFO : collecting all words and their counts\n",
      "2019-12-03 17:41:20,710 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-12-03 17:41:20,711 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-12-03 17:41:20,712 : INFO : Loading a fresh vocabulary\n",
      "2019-12-03 17:41:20,712 : INFO : effective_min_count=5 retains 0 unique words (0% of original 3, drops 3)\n",
      "2019-12-03 17:41:20,713 : INFO : effective_min_count=5 leaves 0 word corpus (0% of original 4, drops 4)\n",
      "2019-12-03 17:41:20,713 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-12-03 17:41:20,714 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2019-12-03 17:41:20,714 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "2019-12-03 17:41:20,715 : INFO : estimated required memory for 0 words and 100 dimensions: 0 bytes\n",
      "2019-12-03 17:41:20,715 : INFO : resetting layer weights\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7a530df70962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'second'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "# sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# model = gensim.models.Word2Vec(sentences, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['Words'])\n",
    "data = df_train['Words'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 17:47:49,515 : INFO : collecting all words and their counts\n",
      "2019-12-03 17:47:49,516 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-12-03 17:47:49,526 : INFO : PROGRESS: at sentence #10000, processed 55417 words, keeping 2552 word types\n",
      "2019-12-03 17:47:49,537 : INFO : PROGRESS: at sentence #20000, processed 116133 words, keeping 4412 word types\n",
      "2019-12-03 17:47:49,549 : INFO : PROGRESS: at sentence #30000, processed 177975 words, keeping 5896 word types\n",
      "2019-12-03 17:47:49,561 : INFO : PROGRESS: at sentence #40000, processed 242641 words, keeping 7110 word types\n",
      "2019-12-03 17:47:49,573 : INFO : PROGRESS: at sentence #50000, processed 308303 words, keeping 8241 word types\n",
      "2019-12-03 17:47:49,585 : INFO : PROGRESS: at sentence #60000, processed 373846 words, keeping 9258 word types\n",
      "2019-12-03 17:47:49,597 : INFO : PROGRESS: at sentence #70000, processed 440950 words, keeping 10205 word types\n",
      "2019-12-03 17:47:49,612 : INFO : PROGRESS: at sentence #80000, processed 509453 words, keeping 11059 word types\n",
      "2019-12-03 17:47:49,625 : INFO : PROGRESS: at sentence #90000, processed 578789 words, keeping 11814 word types\n",
      "2019-12-03 17:47:49,637 : INFO : PROGRESS: at sentence #100000, processed 648440 words, keeping 12578 word types\n",
      "2019-12-03 17:47:49,649 : INFO : PROGRESS: at sentence #110000, processed 717358 words, keeping 13288 word types\n",
      "2019-12-03 17:47:49,663 : INFO : PROGRESS: at sentence #120000, processed 788336 words, keeping 13949 word types\n",
      "2019-12-03 17:47:49,676 : INFO : PROGRESS: at sentence #130000, processed 858576 words, keeping 14653 word types\n",
      "2019-12-03 17:47:49,690 : INFO : PROGRESS: at sentence #140000, processed 929839 words, keeping 15252 word types\n",
      "2019-12-03 17:47:49,706 : INFO : PROGRESS: at sentence #150000, processed 1002817 words, keeping 15875 word types\n",
      "2019-12-03 17:47:49,714 : INFO : collected 16209 word types from a corpus of 1045458 raw words and 156060 sentences\n",
      "2019-12-03 17:47:49,714 : INFO : Loading a fresh vocabulary\n",
      "2019-12-03 17:47:49,759 : INFO : effective_min_count=5 retains 15501 unique words (95% of original 16209, drops 708)\n",
      "2019-12-03 17:47:49,760 : INFO : effective_min_count=5 leaves 1043072 word corpus (99% of original 1045458, drops 2386)\n",
      "2019-12-03 17:47:49,801 : INFO : deleting the raw counts dictionary of 16209 items\n",
      "2019-12-03 17:47:49,802 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2019-12-03 17:47:49,803 : INFO : downsampling leaves estimated 818967 word corpus (78.5% of prior 1043072)\n",
      "2019-12-03 17:47:49,847 : INFO : estimated required memory for 15501 words and 100 dimensions: 20151300 bytes\n",
      "2019-12-03 17:47:49,847 : INFO : resetting layer weights\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  app.launch_new_instance()\n",
      "2019-12-03 17:47:52,376 : INFO : training model with 3 workers on 15501 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-12-03 17:47:52,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-03 17:47:52,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-03 17:47:52,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-03 17:47:52,807 : INFO : EPOCH - 1 : training on 1045458 raw words (818994 effective words) took 0.4s, 1949507 effective words/s\n",
      "2019-12-03 17:47:53,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-03 17:47:53,237 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-03 17:47:53,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-03 17:47:53,239 : INFO : EPOCH - 2 : training on 1045458 raw words (819079 effective words) took 0.4s, 1938461 effective words/s\n",
      "2019-12-03 17:47:53,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-03 17:47:53,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-03 17:47:53,647 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-03 17:47:53,647 : INFO : EPOCH - 3 : training on 1045458 raw words (818652 effective words) took 0.4s, 2051257 effective words/s\n",
      "2019-12-03 17:47:54,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-03 17:47:54,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-03 17:47:54,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-03 17:47:54,047 : INFO : EPOCH - 4 : training on 1045458 raw words (818838 effective words) took 0.4s, 2100044 effective words/s\n",
      "2019-12-03 17:47:54,477 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-03 17:47:54,478 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-03 17:47:54,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-03 17:47:54,482 : INFO : EPOCH - 5 : training on 1045458 raw words (818829 effective words) took 0.4s, 1927333 effective words/s\n",
      "2019-12-03 17:47:54,482 : INFO : training on a 5227290 raw words (4094392 effective words) took 2.1s, 1945437 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4094392, 5227290)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec()\n",
    "model.build_vocab(data)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 18:13:49,983 : INFO : saving Word2Vec object under C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel_word2vec, separately None\n",
      "2019-12-03 18:13:49,984 : INFO : not storing attribute vectors_norm\n",
      "2019-12-03 18:13:49,985 : INFO : not storing attribute cum_table\n",
      "2019-12-03 18:13:50,108 : INFO : saved C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel_word2vec\n"
     ]
    }
   ],
   "source": [
    "model.save(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 17:50:37,721 : INFO : loading Word2Vec object from C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel\n",
      "2019-12-03 17:50:37,825 : INFO : loading trainables recursively from C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel.trainables.* with mmap=None\n",
      "2019-12-03 17:50:37,825 : INFO : loading vocabulary recursively from C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel.vocabulary.* with mmap=None\n",
      "2019-12-03 17:50:37,826 : INFO : loading wv recursively from C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel.wv.* with mmap=None\n",
      "2019-12-03 17:50:37,826 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-12-03 17:50:37,827 : INFO : setting ignored attribute cum_table to None\n",
      "2019-12-03 17:50:37,827 : INFO : loaded C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\MyModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02872228  0.14352956 -0.0189263  -0.15845312  0.07389303  0.00730265\n",
      "  0.04566898 -0.11640306  0.04777636 -0.02660939 -0.02022673  0.14613473\n",
      " -0.13449973  0.01379902 -0.07489165 -0.15723208  0.02523323  0.01254004\n",
      "  0.01964947  0.21682248  0.04079971  0.00635538  0.08565184 -0.12979169\n",
      "  0.11531015 -0.01971537  0.00465549 -0.00655155 -0.01983353 -0.05414752\n",
      "  0.23191294 -0.09314446  0.03435662  0.03271472  0.07035588 -0.05676494\n",
      " -0.051295   -0.12428074 -0.0864871  -0.10262498 -0.00360035 -0.044037\n",
      " -0.01866638 -0.03544784  0.00528329 -0.12180167 -0.01552211  0.06048012\n",
      " -0.04689367 -0.06795652  0.09490557 -0.04505672  0.2005631   0.05330878\n",
      "  0.0102274  -0.09689128  0.07773259  0.07676607 -0.07433616 -0.07535356\n",
      "  0.08545539  0.0216037  -0.03719117 -0.03280009  0.13203709  0.13080849\n",
      " -0.10394157 -0.11951623 -0.09743933 -0.26695287 -0.01621911  0.08761497\n",
      " -0.07325274 -0.06804617 -0.07896756 -0.16531058  0.01478756  0.06169759\n",
      " -0.05267356  0.0046246  -0.02623526  0.11176458 -0.01159136  0.07571731\n",
      "  0.13997379 -0.03797758  0.08959177  0.00246397  0.04744401  0.11778671\n",
      "  0.04648558 -0.02334423 -0.07703646  0.12955695 -0.03837618  0.06877153\n",
      "  0.10088193 -0.01439446  0.09651009  0.05534788]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model['demonstrating'])\n",
    "print(type(model['demonstrating']))\n",
    "len(model['demonstrating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "escapades demonstrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [7328, 3593, 8895, 4503, 10246, 8661, 6401, 74...\n",
       "1    [7328, 3593, 8895, 4503, 10246, 8661, 6401, 74...\n",
       "2                                         [7328, 3593]\n",
       "3                                               [7328]\n",
       "4                                               [3593]\n",
       "Name: Tokens, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the dict to tokenize each phrase\n",
    "df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "df_train['Tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止一切顺利。但是，对于网络的输入，每个短语序列的长度必须相等，因此需要“填充”较短的短语——添加0，使它们的令牌号码长度相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# Step four - get the len of longest phrase\n",
    "max_len = df_train['Tokens'].str.len().max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7328  3593  8895  4503 10246  8661  6401  7484  3577 15393  1211 11050\n",
      "   8661 11189 15393  2892  1211 11050  8661  8210  1524  8895 10982  2856\n",
      "   1172  3915  5986  8895 10982  9142  9593 14927  8895  7328  9805     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 7328  3593  8895  4503 10246  8661  6401  7484  3577 15393  1211 11050\n",
      "   8661 11189     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 7328  3593     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Pad each phrase representation with zeroes, starting from the beginning of sequence\n",
    "# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n",
    "\n",
    "all_tokens = np.array([t for t in df_train['Tokens']])\n",
    "encoded_labels = np.array([l for l in df_train['Sentiment']])\n",
    "\n",
    "# Create blank rows\n",
    "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# for each phrase, add zeros at the end \n",
    "for i, row in enumerate(all_tokens):\n",
    "    features[i, :len(row)] = row\n",
    "\n",
    "#print first 3 values of the feature matrix \n",
    "print(features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features表示156060个词向量，一句话的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分解为训练、验证和测试集，保留80%的训练数据用于训练，剩下的20%平均分配用于验证和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(124848, 48) \n",
      "Validation set: \t(15606, 48) \n",
      "Test set: \t\t(15606, 48)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders和批处理\n",
    "\n",
    "在创建培训、测试和验证数据之后，time top将创建DataLoaders。它们是将数据传递到模型中进行培训/测试的预期方法。加载器是通过以下两个步骤创建的:\n",
    "\n",
    "\n",
    "\n",
    "1)创建一种已知的数据访问格式，使用TensorDataset，它接收具有相同第一个维度的输入数据集和目标数据集，并创建一个数据集。\n",
    "\n",
    "\n",
    "\n",
    "2)创建数据阅读器，批量处理我们的训练、验证和测试张量数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "289\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 54\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Check the size of the loaders (how many batches inside)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个深度网络\n",
    "\n",
    "下面的文字是从另一个摘录文件中借来的，但是它很好地描述了模型构建的方法:\n",
    "\n",
    "\n",
    "\n",
    "首先，我们将文字传递给一个嵌入层。我们需要一个嵌入层，因为我们有成千上万的单词，所以我们需要一个比一个热编码向量更有效的输入数据表示。在这种情况下，嵌入层是为了降维，而不是为了学习语义表示。\n",
    "\n",
    "\n",
    "\n",
    "2)将输入字传递到一个嵌入层后，新的嵌入将被传递到LSTM单元。LSTM细胞将向网络添加周期性连接，并使我们能够在电影评论数据中包含关于单词序列的信息。LSTM接受一个input_size、一个hidden_dim、若干层、一个dropout概率(对于多层之间的dropout)和一个batch_first参数。\n",
    "\n",
    "\n",
    "\n",
    "3)最后，LSTM的输出将进入一个线性层进行最后的分类，再将输出传递给cross-entropy loss function，得到每个预测类的概率。\n",
    "\n",
    "\n",
    "\n",
    "图层如下:\n",
    "\n",
    "\n",
    "\n",
    "将字标记(整数)转换为特定大小的嵌入的嵌入层。\n",
    "\n",
    "由hidden_state大小和层数定义的LSTM层\n",
    "\n",
    "一个完全连接的输出层，它将LSTM层输出映射到所需的output_size\n",
    "\n",
    "一个softmax稍后将应用交叉损耗函数，它将所有输出转换为一个概率\n",
    "\n",
    "多数情况下，网络层数越多，性能越好;在2 - 3之间。添加更多的层允许网络学习真正复杂的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # transform lstm output to input size of linear layers\n",
    "        lstm_out = lstm_out.transpose(0,1)\n",
    "        lstm_out = lstm_out[-1]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)        \n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化网络\n",
    "\n",
    "在这里，我们将实例化网络。首先，定义超参数。\n",
    "\n",
    "\n",
    "\n",
    "vocab_size:词汇表的大小或输入单词标记的值范围。\n",
    "\n",
    "output_size:所需输出的大小;我们想要输出的类分数的数量(0..4)。\n",
    "\n",
    "embedding_dim:嵌入查找表中的列数;我们嵌入的大小。\n",
    "\n",
    "hidden_dim: LSTM细胞隐藏层的单位数量。通常越大性能越好。常用的值是128、256、512等。\n",
    "\n",
    "n_layers:网络中LSTM层的数量。一般在1 - 3之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(16210, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 5\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "培训程序\n",
    "\n",
    "下面是典型的培训代码。由于这是一个多类分类问题，所以将使用交叉损耗。我们也有一些数据和训练超参数:\n",
    "\n",
    "\n",
    "\n",
    "我们的优化器的学习率。\n",
    "\n",
    "epochs:遍历训练数据集的次数。\n",
    "\n",
    "剪辑:最大的渐变值剪辑在(防止爆炸的梯度)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.003\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(16210, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(16210, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6a222e28a323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[1;31m# get the output from the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[1;31m# calculate the loss and perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-557f55113e8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;31m# embeddings and lstm_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, labels)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(16210, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7277f38f40a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[1;31m# get predicted outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-557f55113e8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;31m# embeddings and lstm_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output,1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
