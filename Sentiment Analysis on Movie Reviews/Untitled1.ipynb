{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "print(os.listdir(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\\train.tsv\", sep='\\t')\n",
    "test = pd.read_csv(r\"C:\\Users\\dongy\\Sentiment Analysis on Movie Reviews\\dataset\\test.tsv\", sep='\\t')\n",
    "train['Sentiment'] = train['Sentiment'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "  Sentiment  \n",
       "0         1  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_id = test['PhraseId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An intermittently pleasing but mostly routine effort .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', '4', '0'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1650'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = train.Phrase.values\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = train.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:03<00:00, 71005.49B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'a', 'series', 'of', 'es', '##cap', '##ades', 'demonstrating', 'the', 'ada', '##ge', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gan', '##der', ',', 'some', 'of', 'which', 'occasionally', 'am', '##uses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-47e485babd34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mdistplot\u001b[0;34m(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mkde_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkde_kws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mkdeplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvertical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkde_color\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkde_kws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkde_color\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mkde_kws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkde_color\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mkdeplot\u001b[0;34m(data, data2, shade, vertical, kernel, bw, gridsize, cut, clip, legend, cumulative, shade_lowest, cbar, cbar_ax, cbar_kws, ax, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m         ax = _univariate_kdeplot(data, shade, vertical, kernel, bw,\n\u001b[1;32m    690\u001b[0m                                  \u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                                  cumulative=cumulative, **kwargs)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36m_univariate_kdeplot\u001b[0;34m(data, shade, vertical, kernel, bw, gridsize, cut, clip, legend, ax, cumulative, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m         x, y = _statsmodels_univariate_kde(data, kernel, bw,\n\u001b[1;32m    282\u001b[0m                                            \u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                                            cumulative=cumulative)\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[1;31m# Fall back to scipy if missing statsmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36m_statsmodels_univariate_kde\u001b[0;34m(data, kernel, bw, gridsize, cut, clip, cumulative)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mfft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gau\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mkde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKDEUnivariate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, kernel, bw, fft, weights, gridsize, adjust, cut, clip)\u001b[0m\n\u001b[1;32m    144\u001b[0m             density, grid, bw = kdensityfft(endog, kernel=kernel, bw=bw,\n\u001b[1;32m    145\u001b[0m                     \u001b[0madjust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjust\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                     clip=clip, cut=cut)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             density, grid, bw = kdensity(endog, kernel=kernel, bw=bw,\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py\u001b[0m in \u001b[0;36mkdensityfft\u001b[0;34m(X, kernel, bw, weights, gridsize, adjust, clip, cut, retgrid)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mzstar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilverman_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRANGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m \u001b[1;31m# 3.49 in Silverman\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                                                    \u001b[1;31m# 3.50 w Gaussian kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrevrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzstar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretgrid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\statsmodels\\nonparametric\\kdetools.py\u001b[0m in \u001b[0;36mrevrt\u001b[0;34m(X, m)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1j\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mirfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXiFYvDsn5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21cdb4de4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train.Phrase.apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 32\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 194795095/407873900 [35:50<39:12, 90579.37B/s]  \n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bdb9cab3448a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m                 resolved_archive_file, tempdir))\n\u001b[1;32m    589\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r:gz'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                 \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mserialization_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[1;31m# Load config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[1;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0;32m-> 1998\u001b[0;31m                          numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[1;31m# Reverse sort directories.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2038\u001b[0m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[1;32m   2039\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                                  numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2041\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorlevel\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2156\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBUFSIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUFSIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mBUFSIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dongy\\Anaconda3\\envs\\pytorch\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    481\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = test.Phrase.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "# labels = test.Sentiment.values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 32\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Sentiment'] = 0\n",
    "labels = test.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 16  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "#   true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preds, target = learn_classifier.get_preds(DatasetType.Test, ordered=True)\n",
    "labels = np.argmax(preds, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PhraseId': test_id, 'Sentiment': labels})\n",
    "submission.to_csv('submission_torch_bert.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
