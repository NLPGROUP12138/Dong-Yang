{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 104805 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print (\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们编写的用于清理数据的函数也类似于第1部分，尽管现在有一些不同之处。首先，为了训练Word2Vec，**最好不要删除停止词，因为为了生成高质量的词向量，该算法依赖于句子的更广泛的上下文。**出于这个原因，我们将在下面的函数中使stop单词删除成为**可选的**。不删除数字可能更好，但是我们把它留给读者作为练习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML 删除html符号\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters 删除非字母符号，后续可以考虑不删除数字\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them 把所有单词转换成小写然后将文本分割成单词\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default) 有选择的删除stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要一个特定的输入格式。**Word2Vec每个句子都是以单词为元素列表，文本是以句子为元素的列表，其中，橘子也是一个列表。换句话说，输入格式是列表的列表。**\n",
    "\n",
    "\n",
    "如何把一段话分成句子一点也不简单。在自然语言中有各种各样的陷阱。英语句子可以以“?”结尾,“!”\"\"\"或\"。此外，间距和标题化也不是可靠的指导。出于这个原因，我们将使用NLTK的punkt标记器进行句子拆分。为了使用它，您需要安装NLTK并使用NLTK .download()来下载punkt的相关训练文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "#nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#print(tokenizer)\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    # 函数的作用是:将review分解成已解析的句子。返回一个句子列表，其中每个句子都是单词列表\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence遍历\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以应用这个函数来准备我们的数据输入到Word2Vec(这将需要几分钟):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:302: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:375: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c0833954352c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parsing sentences from unlabeled set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munlabeled_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreview_to_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-9518b57b4611>\u001b[0m in \u001b[0;36mreview_to_sentences\u001b[1;34m(review, tokenizer, remove_stopwords)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 函数的作用是:将review分解成已解析的句子。返回一个句子列表，其中每个句子都是单词列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 1. Use the NLTK tokenizer to split the paragraph into sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mraw_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# 2. Loop over each sentence遍历\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能会从BeautifulSoup那里得到一些关于句子中url的警告。这些都不需要担心(尽管您可能想在清理文本时删除url)。\n",
    "\n",
    "\n",
    "我们可以看看输出，看看这与第1部分有什么不同:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704530\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total - should be around 850,000+\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的一个小细节是Python列表中“+=”和“append”之间的区别。在许多应用程序中，这两者是可互换的，但在这里就不一样了。**如果你将一个列表的列表附加到另一个列表的列表中，“附加”只会附加第一个列表;您需要使用“+=”来一次性连接所有列表。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了这些解析良好的句子列表，我们就可以开始训练模型了。有许多参数选择会影响运行时和生成的最终模型的质量。有关下面算法的详细信息，请参阅word2vec API文档和谷歌文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**架构**:架构选项是跳跃图(默认)或连续的单词包。我们发现跳跃图的速度稍微慢一些，但效果更好。\n",
    "\n",
    "**训练算法**:分级softmax(默认)或消极抽样。对我们来说，默认设置运行良好。\n",
    "\n",
    "**对常用词进行下采样**:谷歌文档建议的值在.00001和.001之间。对于我们来说，接近0.001的值似乎可以提高最终模型的准确性。\n",
    "\n",
    "**字向量维度**:更多的特征导致更长的运行时，并且通常(但不总是)导致更好的模型。合理的数值可以是几十到几百;我们使用300年。\n",
    "\n",
    "**上下文/窗口大小**:训练算法应该考虑多少上下文单词?选择10在分层softmax中效果不错(更多是更好的，在一定程度上)。\n",
    "\n",
    "**工作线程**:要运行的并行进程的数量。这是特定于计算机的，但是在大多数系统中4到6之间应该可以工作。\n",
    "\n",
    "**最少字数**:这有助于将词汇量限制在有意义的单词上。任何在所有文档中至少出现这么多次的单词都将被忽略。合理的数值可以在10到100之间。在本例中，由于每部电影出现30次，因此我们将最小字数设置为40，以避免对单个电影标题赋予过多的重要性。结果，总的词汇量大约是15000个单词。更高的值还有助于限制运行时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择参数并不容易，但是一旦我们选择了参数，创建一个Word2Vec模型就很简单了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:58:30,351 : INFO : collecting all words and their counts\n",
      "2019-10-08 19:58:30,351 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-08 19:58:30,414 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-10-08 19:58:30,457 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-10-08 19:58:30,509 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:58:30,560 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2019-10-08 19:58:30,598 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2019-10-08 19:58:30,635 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2019-10-08 19:58:30,684 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2019-10-08 19:58:30,731 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2019-10-08 19:58:30,779 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2019-10-08 19:58:30,829 : INFO : PROGRESS: at sentence #100000, processed 2226967 words, keeping 50207 word types\n",
      "2019-10-08 19:58:30,875 : INFO : PROGRESS: at sentence #110000, processed 2446581 words, keeping 52081 word types\n",
      "2019-10-08 19:58:30,907 : INFO : PROGRESS: at sentence #120000, processed 2668776 words, keeping 54119 word types\n",
      "2019-10-08 19:58:30,961 : INFO : PROGRESS: at sentence #130000, processed 2894304 words, keeping 55847 word types\n",
      "2019-10-08 19:58:31,000 : INFO : PROGRESS: at sentence #140000, processed 3107006 words, keeping 57346 word types\n",
      "2019-10-08 19:58:31,045 : INFO : PROGRESS: at sentence #150000, processed 3332628 words, keeping 59055 word types\n",
      "2019-10-08 19:58:31,093 : INFO : PROGRESS: at sentence #160000, processed 3555316 words, keeping 60617 word types\n",
      "2019-10-08 19:58:31,124 : INFO : PROGRESS: at sentence #170000, processed 3778656 words, keeping 62077 word types\n",
      "2019-10-08 19:58:31,168 : INFO : PROGRESS: at sentence #180000, processed 3999237 words, keeping 63496 word types\n",
      "2019-10-08 19:58:31,213 : INFO : PROGRESS: at sentence #190000, processed 4224450 words, keeping 64794 word types\n",
      "2019-10-08 19:58:31,275 : INFO : PROGRESS: at sentence #200000, processed 4448604 words, keeping 66087 word types\n",
      "2019-10-08 19:58:31,316 : INFO : PROGRESS: at sentence #210000, processed 4669968 words, keeping 67390 word types\n",
      "2019-10-08 19:58:31,370 : INFO : PROGRESS: at sentence #220000, processed 4894969 words, keeping 68697 word types\n",
      "2019-10-08 19:58:31,405 : INFO : PROGRESS: at sentence #230000, processed 5117546 words, keeping 69958 word types\n",
      "2019-10-08 19:58:31,450 : INFO : PROGRESS: at sentence #240000, processed 5345051 words, keeping 71167 word types\n",
      "2019-10-08 19:58:31,481 : INFO : PROGRESS: at sentence #250000, processed 5559166 words, keeping 72351 word types\n",
      "2019-10-08 19:58:31,531 : INFO : PROGRESS: at sentence #260000, processed 5779147 words, keeping 73478 word types\n",
      "2019-10-08 19:58:31,584 : INFO : PROGRESS: at sentence #270000, processed 6000436 words, keeping 74767 word types\n",
      "2019-10-08 19:58:31,620 : INFO : PROGRESS: at sentence #280000, processed 6228005 words, keeping 76318 word types\n",
      "2019-10-08 19:58:31,669 : INFO : PROGRESS: at sentence #290000, processed 6453074 words, keeping 77866 word types\n",
      "2019-10-08 19:58:31,723 : INFO : PROGRESS: at sentence #300000, processed 6676753 words, keeping 79354 word types\n",
      "2019-10-08 19:58:31,764 : INFO : PROGRESS: at sentence #310000, processed 6899292 words, keeping 80644 word types\n",
      "2019-10-08 19:58:31,801 : INFO : PROGRESS: at sentence #320000, processed 7123700 words, keeping 81908 word types\n",
      "2019-10-08 19:58:31,847 : INFO : PROGRESS: at sentence #330000, processed 7349057 words, keeping 83159 word types\n",
      "2019-10-08 19:58:31,883 : INFO : PROGRESS: at sentence #340000, processed 7578455 words, keeping 84350 word types\n",
      "2019-10-08 19:58:31,932 : INFO : PROGRESS: at sentence #350000, processed 7802604 words, keeping 85485 word types\n",
      "2019-10-08 19:58:31,977 : INFO : PROGRESS: at sentence #360000, processed 8028011 words, keeping 86583 word types\n",
      "2019-10-08 19:58:32,016 : INFO : PROGRESS: at sentence #370000, processed 8252739 words, keeping 87731 word types\n",
      "2019-10-08 19:58:32,061 : INFO : PROGRESS: at sentence #380000, processed 8475180 words, keeping 88819 word types\n",
      "2019-10-08 19:58:32,112 : INFO : PROGRESS: at sentence #390000, processed 8695297 words, keeping 89847 word types\n",
      "2019-10-08 19:58:32,159 : INFO : PROGRESS: at sentence #400000, processed 8916919 words, keeping 90929 word types\n",
      "2019-10-08 19:58:32,206 : INFO : PROGRESS: at sentence #410000, processed 9142578 words, keeping 92004 word types\n",
      "2019-10-08 19:58:32,262 : INFO : PROGRESS: at sentence #420000, processed 9367682 words, keeping 93037 word types\n",
      "2019-10-08 19:58:32,313 : INFO : PROGRESS: at sentence #430000, processed 9596645 words, keeping 94072 word types\n",
      "2019-10-08 19:58:32,352 : INFO : PROGRESS: at sentence #440000, processed 9821675 words, keeping 95070 word types\n",
      "2019-10-08 19:58:32,400 : INFO : PROGRESS: at sentence #450000, processed 10044014 words, keeping 95986 word types\n",
      "2019-10-08 19:58:32,432 : INFO : PROGRESS: at sentence #460000, processed 10266646 words, keeping 96924 word types\n",
      "2019-10-08 19:58:32,479 : INFO : PROGRESS: at sentence #470000, processed 10496459 words, keeping 97908 word types\n",
      "2019-10-08 19:58:32,526 : INFO : PROGRESS: at sentence #480000, processed 10722189 words, keeping 98737 word types\n",
      "2019-10-08 19:58:32,568 : INFO : PROGRESS: at sentence #490000, processed 10947488 words, keeping 99650 word types\n",
      "2019-10-08 19:58:32,611 : INFO : PROGRESS: at sentence #500000, processed 11169412 words, keeping 100553 word types\n",
      "2019-10-08 19:58:32,660 : INFO : PROGRESS: at sentence #510000, processed 11393217 words, keeping 101492 word types\n",
      "2019-10-08 19:58:32,706 : INFO : PROGRESS: at sentence #520000, processed 11614950 words, keeping 102407 word types\n",
      "2019-10-08 19:58:32,745 : INFO : PROGRESS: at sentence #530000, processed 11839605 words, keeping 103300 word types\n",
      "2019-10-08 19:58:32,794 : INFO : PROGRESS: at sentence #540000, processed 12067387 words, keeping 104201 word types\n",
      "2019-10-08 19:58:32,851 : INFO : PROGRESS: at sentence #550000, processed 12293117 words, keeping 104970 word types\n",
      "2019-10-08 19:58:32,898 : INFO : PROGRESS: at sentence #560000, processed 12520313 words, keeping 105840 word types\n",
      "2019-10-08 19:58:32,932 : INFO : PROGRESS: at sentence #570000, processed 12747339 words, keeping 106062 word types\n",
      "2019-10-08 19:58:32,988 : INFO : PROGRESS: at sentence #580000, processed 12970768 words, keeping 106755 word types\n",
      "2019-10-08 19:58:33,033 : INFO : PROGRESS: at sentence #590000, processed 13197134 words, keeping 107356 word types\n",
      "2019-10-08 19:58:33,084 : INFO : PROGRESS: at sentence #600000, processed 13417045 words, keeping 107659 word types\n",
      "2019-10-08 19:58:33,116 : INFO : PROGRESS: at sentence #610000, processed 13636640 words, keeping 108476 word types\n",
      "2019-10-08 19:58:33,163 : INFO : PROGRESS: at sentence #620000, processed 13861829 words, keeping 108677 word types\n",
      "2019-10-08 19:58:33,210 : INFO : PROGRESS: at sentence #630000, processed 14092584 words, keeping 109496 word types\n",
      "2019-10-08 19:58:33,255 : INFO : PROGRESS: at sentence #640000, processed 14317604 words, keeping 110123 word types\n",
      "2019-10-08 19:58:33,298 : INFO : PROGRESS: at sentence #650000, processed 14543681 words, keeping 110467 word types\n",
      "2019-10-08 19:58:33,345 : INFO : PROGRESS: at sentence #660000, processed 14772587 words, keeping 111294 word types\n",
      "2019-10-08 19:58:33,383 : INFO : PROGRESS: at sentence #670000, processed 14989781 words, keeping 111407 word types\n",
      "2019-10-08 19:58:33,447 : INFO : PROGRESS: at sentence #680000, processed 15217521 words, keeping 112093 word types\n",
      "2019-10-08 19:58:33,490 : INFO : PROGRESS: at sentence #690000, processed 15439969 words, keeping 112634 word types\n",
      "2019-10-08 19:58:33,548 : INFO : PROGRESS: at sentence #700000, processed 15661493 words, keeping 113062 word types\n",
      "2019-10-08 19:58:33,577 : INFO : collected 113458 word types from a corpus of 15762105 raw words and 704530 sentences\n",
      "2019-10-08 19:58:33,577 : INFO : Loading a fresh vocabulary\n",
      "2019-10-08 19:58:33,663 : INFO : effective_min_count=40 retains 15440 unique words (13% of original 113458, drops 98018)\n",
      "2019-10-08 19:58:33,663 : INFO : effective_min_count=40 leaves 15230444 word corpus (96% of original 15762105, drops 531661)\n",
      "2019-10-08 19:58:33,699 : INFO : deleting the raw counts dictionary of 113458 items\n",
      "2019-10-08 19:58:33,708 : INFO : sample=0.001 downsamples 49 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:58:33,708 : INFO : downsampling leaves estimated 11248894 word corpus (73.9% of prior 15230444)\n",
      "2019-10-08 19:58:33,764 : INFO : estimated required memory for 15440 words and 300 dimensions: 44776000 bytes\n",
      "2019-10-08 19:58:33,764 : INFO : resetting layer weights\n",
      "2019-10-08 19:58:33,954 : INFO : training model with 4 workers on 15440 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-10-08 19:58:34,967 : INFO : EPOCH 1 - PROGRESS: at 10.81% examples, 1206145 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:58:35,978 : INFO : EPOCH 1 - PROGRESS: at 22.70% examples, 1263853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:36,981 : INFO : EPOCH 1 - PROGRESS: at 34.92% examples, 1297021 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:37,967 : INFO : EPOCH 1 - PROGRESS: at 47.44% examples, 1324783 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:38,985 : INFO : EPOCH 1 - PROGRESS: at 60.09% examples, 1343685 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:58:39,987 : INFO : EPOCH 1 - PROGRESS: at 72.80% examples, 1357841 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:40,990 : INFO : EPOCH 1 - PROGRESS: at 85.30% examples, 1365186 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:58:41,992 : INFO : EPOCH 1 - PROGRESS: at 97.95% examples, 1372311 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:42,148 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-08 19:58:42,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-08 19:58:42,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-08 19:58:42,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-08 19:58:42,160 : INFO : EPOCH - 1 : training on 15762105 raw words (11247685 effective words) took 8.2s, 1373262 effective words/s\n",
      "2019-10-08 19:58:43,166 : INFO : EPOCH 2 - PROGRESS: at 12.33% examples, 1378803 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:44,168 : INFO : EPOCH 2 - PROGRESS: at 25.14% examples, 1398998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:45,171 : INFO : EPOCH 2 - PROGRESS: at 37.82% examples, 1404218 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:46,173 : INFO : EPOCH 2 - PROGRESS: at 49.89% examples, 1392890 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:47,179 : INFO : EPOCH 2 - PROGRESS: at 61.32% examples, 1370777 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:48,185 : INFO : EPOCH 2 - PROGRESS: at 72.74% examples, 1356867 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:49,181 : INFO : EPOCH 2 - PROGRESS: at 84.63% examples, 1355349 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:50,186 : INFO : EPOCH 2 - PROGRESS: at 97.25% examples, 1363313 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:50,386 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-08 19:58:50,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-08 19:58:50,394 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-08 19:58:50,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-08 19:58:50,403 : INFO : EPOCH - 2 : training on 15762105 raw words (11249172 effective words) took 8.2s, 1365403 effective words/s\n",
      "2019-10-08 19:58:51,403 : INFO : EPOCH 3 - PROGRESS: at 12.45% examples, 1396039 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:52,406 : INFO : EPOCH 3 - PROGRESS: at 25.20% examples, 1402545 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:53,415 : INFO : EPOCH 3 - PROGRESS: at 37.76% examples, 1402368 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:54,406 : INFO : EPOCH 3 - PROGRESS: at 50.07% examples, 1400196 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:55,410 : INFO : EPOCH 3 - PROGRESS: at 62.72% examples, 1404847 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:56,416 : INFO : EPOCH 3 - PROGRESS: at 75.10% examples, 1403314 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:57,411 : INFO : EPOCH 3 - PROGRESS: at 87.53% examples, 1402589 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:58:58,399 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-08 19:58:58,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-08 19:58:58,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-08 19:58:58,414 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-08 19:58:58,414 : INFO : EPOCH - 3 : training on 15762105 raw words (11248531 effective words) took 8.0s, 1405182 effective words/s\n",
      "2019-10-08 19:58:59,416 : INFO : EPOCH 4 - PROGRESS: at 10.68% examples, 1196009 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:59:00,419 : INFO : EPOCH 4 - PROGRESS: at 22.70% examples, 1267244 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:59:01,424 : INFO : EPOCH 4 - PROGRESS: at 35.49% examples, 1319266 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:02,420 : INFO : EPOCH 4 - PROGRESS: at 48.12% examples, 1344619 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:03,434 : INFO : EPOCH 4 - PROGRESS: at 58.65% examples, 1311463 words/s, in_qsize 7, out_qsize 1\n",
      "2019-10-08 19:59:04,435 : INFO : EPOCH 4 - PROGRESS: at 68.93% examples, 1286047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:05,436 : INFO : EPOCH 4 - PROGRESS: at 80.41% examples, 1286596 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:06,447 : INFO : EPOCH 4 - PROGRESS: at 90.95% examples, 1274022 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:07,248 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-08 19:59:07,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-08 19:59:07,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-08 19:59:07,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-08 19:59:07,265 : INFO : EPOCH - 4 : training on 15762105 raw words (11248690 effective words) took 8.9s, 1270411 effective words/s\n",
      "2019-10-08 19:59:08,281 : INFO : EPOCH 5 - PROGRESS: at 10.94% examples, 1224941 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:09,283 : INFO : EPOCH 5 - PROGRESS: at 22.58% examples, 1259316 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:59:10,287 : INFO : EPOCH 5 - PROGRESS: at 33.98% examples, 1266405 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:59:11,288 : INFO : EPOCH 5 - PROGRESS: at 45.89% examples, 1280953 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:12,295 : INFO : EPOCH 5 - PROGRESS: at 57.51% examples, 1286412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:13,301 : INFO : EPOCH 5 - PROGRESS: at 69.07% examples, 1289770 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:14,304 : INFO : EPOCH 5 - PROGRESS: at 80.59% examples, 1290634 words/s, in_qsize 8, out_qsize 0\n",
      "2019-10-08 19:59:15,305 : INFO : EPOCH 5 - PROGRESS: at 92.07% examples, 1291399 words/s, in_qsize 7, out_qsize 0\n",
      "2019-10-08 19:59:15,938 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-08 19:59:15,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-08 19:59:15,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-08 19:59:15,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-08 19:59:15,964 : INFO : EPOCH - 5 : training on 15762105 raw words (11249523 effective words) took 8.7s, 1296242 effective words/s\n",
      "2019-10-08 19:59:15,964 : INFO : training on a 78810525 raw words (56243601 effective words) took 42.0s, 1339103 effective words/s\n",
      "2019-10-08 19:59:15,964 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-10-08 19:59:15,980 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-10-08 19:59:15,980 : INFO : not storing attribute vectors_norm\n",
      "2019-10-08 19:59:15,984 : INFO : not storing attribute cum_table\n",
      "2019-10-08 19:59:16,303 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages 导入内置的日志模块并对其进行配置，以便Word2Vec创建良好的输出消息\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel 要并行运行的线程数\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words 下采样设置频繁的单词\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\") \n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "祝贺你到目前为止一切顺利!让我们来看看我们从75000个培训评论中创建的模型。\n",
    "\n",
    "\n",
    "“doesnt_match”函数将尝试推断出一个集合中哪些单词与其他单词最不相似:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的模型能够区分意义上的差异!它知道，男人、女人和孩子之间的相似之处比他们在厨房里的相似之处更多。更多的探索表明，该模型对更细微的意义差异更敏感，例如国家和城市之间的差异:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...虽然我们使用的训练集相对较小，但它肯定不是完美的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以使用“most_similar”函数来深入了解模型的词簇:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6359063982963562),\n",
       " ('lady', 0.5870093703269958),\n",
       " ('lad', 0.5420260429382324),\n",
       " ('monk', 0.5376467108726501),\n",
       " ('boy', 0.5312069654464722),\n",
       " ('priest', 0.5237777233123779),\n",
       " ('person', 0.5219061374664307),\n",
       " ('men', 0.5074207782745361),\n",
       " ('millionaire', 0.5048858523368835),\n",
       " ('guy', 0.4964485168457031)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6752606630325317),\n",
       " ('bride', 0.6285693645477295),\n",
       " ('fatale', 0.5853664875030518),\n",
       " ('regina', 0.580701470375061),\n",
       " ('femme', 0.5714904069900513),\n",
       " ('maid', 0.5712674260139465),\n",
       " ('belle', 0.5650311708450317),\n",
       " ('mistress', 0.5605853796005249),\n",
       " ('sultry', 0.5578803420066833),\n",
       " ('dame', 0.5563564300537109)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑到我们的特殊训练集，《辣提法》与《女王》的相似度高居榜首也就不足为奇了。\n",
    "\n",
    "\n",
    "或者，与情绪分析更相关的是:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7532333731651306),\n",
       " ('horrible', 0.7510112524032593),\n",
       " ('dreadful', 0.707053542137146),\n",
       " ('atrocious', 0.7025836706161499),\n",
       " ('abysmal', 0.6978950500488281),\n",
       " ('appalling', 0.6969230771064758),\n",
       " ('horrendous', 0.6883354187011719),\n",
       " ('horrid', 0.6818529367446899),\n",
       " ('lousy', 0.6378222703933716),\n",
       " ('embarrassing', 0.6169452667236328)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们似乎有了一个相当好的语义模型——至少和单词一样好。但是我们如何使用这些神奇的分布式单词向量进行监督学习呢?下一节将对此进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Representations of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "现在我们已经有了一个训练有素的模型，对单词有了一定的语义理解，那么我们应该如何使用它呢?在第2部分中训练的Word2Vec模型由词汇表中每个单词的特征向量组成，存储在一个名为“syn0”的numpy数组中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:59:59,496 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2019-10-08 19:59:59,734 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2019-10-08 19:59:59,734 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2019-10-08 19:59:59,734 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2019-10-08 19:59:59,734 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-10-08 19:59:59,737 : INFO : setting ignored attribute cum_table to None\n",
      "2019-10-08 19:59:59,737 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c05b54887e74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-8454ce32d988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "syn0中的行数是模型词汇表中的单词数，列数对应于我们在第2部分中设置的特征向量的大小。将最小字数设置为40，我们得到的总词汇量为16,492个单词，每个单词有300个特征。个别字向量的存取方法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -8.59023705e-02,  -3.39425132e-02,  -7.29977340e-02,\n",
       "        -3.47801521e-02,  -3.87294665e-02,   6.23288602e-02,\n",
       "         1.45014273e-02,   2.29095016e-02,   4.18861061e-02,\n",
       "        -4.63192957e-03,   6.92662969e-02,   3.67813110e-02,\n",
       "        -2.85203774e-02,  -7.00469762e-02,  -2.33646948e-02,\n",
       "        -8.65339935e-02,  -1.02313928e-01,   9.40788761e-02,\n",
       "        -3.46443467e-02,   1.28474422e-02,  -2.49164477e-02,\n",
       "        -5.74302906e-03,   2.05675513e-02,   1.33091249e-02,\n",
       "         3.76211889e-02,   2.06470024e-02,   3.52376513e-02,\n",
       "         2.17546653e-02,  -1.05265353e-03,   2.71668136e-02,\n",
       "         8.24769586e-02,  -6.59254147e-03,  -1.90616082e-02,\n",
       "         3.39015061e-03,  -1.57508515e-02,   7.15400651e-02,\n",
       "         4.43138881e-03,  -6.99389949e-02,   1.15690805e-01,\n",
       "        -3.39600183e-02,  -9.79051590e-02,  -7.17130443e-03,\n",
       "         2.42847041e-03,   1.04541451e-01,   1.85147729e-02,\n",
       "        -3.89136374e-02,   3.08787171e-02,   8.55970308e-02,\n",
       "         8.79267603e-02,  -3.24270385e-03,  -8.13802890e-03,\n",
       "        -7.10349008e-02,  -5.47676869e-02,   4.19531167e-02,\n",
       "         8.78685340e-03,  -1.66536414e-03,  -2.89079379e-02,\n",
       "         2.82255411e-02,  -8.67122412e-02,  -2.90128719e-02,\n",
       "        -9.78293922e-03,  -1.28978547e-02,   3.88098806e-02,\n",
       "        -1.08182682e-02,   5.39516732e-02,   4.22728173e-02,\n",
       "        -8.41978565e-03,  -3.48001383e-02,  -4.67971899e-03,\n",
       "         2.50095669e-02,  -7.84969926e-02,   1.10666715e-02,\n",
       "        -6.48267120e-02,   6.81505874e-02,   3.63309495e-02,\n",
       "         4.06913944e-02,  -6.46444708e-02,   6.68586046e-02,\n",
       "         5.77158779e-02,   1.05349451e-01,   4.40910310e-02,\n",
       "         8.19793269e-02,   2.72218380e-02,  -5.40737547e-02,\n",
       "        -9.94080976e-02,  -3.09561938e-02,  -1.03693299e-01,\n",
       "        -2.36823652e-02,   9.31692868e-02,   2.51157135e-02,\n",
       "         5.31882532e-02,   1.15047395e-02,   2.04602554e-02,\n",
       "         6.05676919e-02,   7.81886000e-03,  -2.37316024e-02,\n",
       "         6.64510801e-02,  -7.75927082e-02,   3.84922773e-02,\n",
       "        -7.10094348e-03,  -2.58893184e-02,   1.55692929e-02,\n",
       "         1.19310901e-01,   7.29202032e-02,  -1.04483098e-01,\n",
       "        -1.59141064e-01,  -1.30407661e-02,   5.17126843e-02,\n",
       "        -6.81851059e-03,   3.31600048e-02,  -2.65730154e-02,\n",
       "        -4.62910160e-02,  -3.74137014e-02,   5.91953238e-03,\n",
       "        -1.85902347e-03,  -8.05231482e-02,   1.51371136e-02,\n",
       "        -7.94253424e-02,  -1.06173605e-01,   3.29559259e-02,\n",
       "        -8.63268226e-03,  -6.56920969e-02,   2.81152944e-03,\n",
       "        -8.99445340e-02,  -3.87231633e-02,  -1.73737742e-02,\n",
       "        -5.75496873e-04,  -2.65118312e-02,   3.83434966e-02,\n",
       "         1.22501932e-01,   5.81958471e-03,  -1.84787456e-02,\n",
       "         5.61773852e-02,   7.83065110e-02,  -5.62804155e-02,\n",
       "        -6.75662011e-02,   6.45301044e-02,  -2.82619130e-02,\n",
       "        -5.42705879e-04,   3.59443910e-02,  -1.02152331e-02,\n",
       "        -1.00988699e-02,   2.88752955e-03,   3.65179703e-02,\n",
       "        -1.38447396e-02,  -3.91051173e-02,  -1.35027915e-02,\n",
       "         2.54416019e-02,  -3.65802124e-02,   1.66527145e-02,\n",
       "        -2.61621233e-02,   2.57009063e-02,   2.58122329e-02,\n",
       "        -2.27068155e-03,   4.80948575e-02,  -3.29923108e-02,\n",
       "        -3.75398509e-02,  -6.58764914e-02,   4.88032289e-02,\n",
       "         3.45821418e-02,  -1.94502585e-02,   4.94437069e-02,\n",
       "        -3.57584953e-02,  -4.59050983e-02,   5.99228591e-02,\n",
       "        -2.90745795e-02,   1.66485924e-02,   1.30394073e-02,\n",
       "         1.01566985e-01,  -2.33254805e-02,  -1.33758159e-02,\n",
       "         2.97732260e-02,   3.81489880e-02,   1.05226273e-02,\n",
       "         6.44647852e-02,   3.79152857e-02,  -5.20622581e-02,\n",
       "        -9.45749283e-02,  -5.53622423e-03,   1.67619754e-02,\n",
       "         2.58387383e-02,  -5.26261218e-02,  -5.52489981e-02,\n",
       "        -2.15444304e-02,   8.72193268e-05,  -3.41970809e-02,\n",
       "         6.09817393e-02,  -4.43641730e-02,   3.13907005e-02,\n",
       "         5.10954717e-03,  -5.95938936e-02,   6.92152977e-02,\n",
       "        -3.84053476e-02,  -9.97621268e-02,   6.33075237e-02,\n",
       "         1.02289952e-01,   1.14227273e-02,   1.63037702e-02,\n",
       "        -5.99425733e-02,   6.55423291e-03,   3.62825021e-02,\n",
       "        -1.83364689e-01,   4.90506962e-02,  -2.33741105e-02,\n",
       "         4.54018861e-02,   6.11134209e-02,   2.49095093e-02,\n",
       "         7.21808895e-02,   3.81799191e-02,  -5.31015471e-02,\n",
       "         5.42425783e-04,   6.62422739e-03,  -4.41258326e-02,\n",
       "        -9.28610787e-02,  -1.08025022e-01,   8.83696005e-02,\n",
       "         1.23303957e-01,   8.94373208e-02,   1.16622411e-01,\n",
       "         1.90982036e-02,   6.71260655e-02,   5.85347973e-02,\n",
       "         6.46443069e-02,   2.50605047e-02,  -1.15076251e-01,\n",
       "        -3.31802405e-02,  -8.50075781e-02,   9.29556265e-02,\n",
       "         5.21676540e-02,   7.21846446e-02,   5.29624475e-03,\n",
       "        -2.57571191e-02,  -6.33254126e-02,   2.04211008e-02,\n",
       "        -4.90689166e-02,   8.12309831e-02,  -9.92268845e-02,\n",
       "         9.61812735e-02,  -3.51172360e-03,  -6.20303501e-04,\n",
       "         2.38304771e-02,   1.42362705e-02,   1.91673283e-02,\n",
       "         6.84421808e-02,   1.36673392e-03,   4.34004143e-02,\n",
       "        -1.32885929e-02,   7.51046762e-02,   6.96864128e-02,\n",
       "         2.22690906e-02,  -1.71137098e-02,   6.35967851e-02,\n",
       "        -9.53816250e-02,   4.16653454e-02,   9.17961821e-03,\n",
       "        -4.57829833e-02,  -6.08259998e-02,  -2.47573294e-02,\n",
       "         8.44428614e-02,   5.59868105e-02,  -3.48563679e-03,\n",
       "         5.72209470e-02,  -1.91613752e-02,   1.64341682e-03,\n",
       "        -1.29336994e-02,  -8.72869566e-02,   4.48215678e-02,\n",
       "        -5.07176779e-02,  -1.05596617e-01,  -3.68042253e-02,\n",
       "         9.67709050e-02,   7.72116110e-02,   1.12894615e-02,\n",
       "         7.64429048e-02,   6.02689646e-02,  -2.44822702e-03,\n",
       "         5.42634912e-02,   1.54096382e-02,   1.51282892e-01,\n",
       "        -2.11710379e-01,  -2.65944778e-04,  -1.00837946e-02,\n",
       "         1.07977375e-01,   5.04866578e-02,  -6.78091943e-02,\n",
       "         1.38123214e-01,   6.14238298e-03,  -5.37441112e-02,\n",
       "        -3.17315059e-03,   8.82473867e-03,  -6.96570724e-02,\n",
       "         7.83305168e-02,  -8.72105658e-02,  -1.02715835e-01,\n",
       "         3.58983092e-02,   7.78003177e-03,   3.32309119e-02,\n",
       "        -5.23755588e-02,   6.09676428e-02,  -8.21983740e-02], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB数据集的一个挑战是可变长度的审查。我们需要找到一种方法来提取单个的单词向量，并将它们转换成一个特征集，每个特征集的长度都是相同的。\n",
    "\n",
    "\n",
    "由于每个单词都是300维空间中的向量，所以我们可以使用向量操作来组合每个review中的单词。我们尝试的一种方法是在给定的review中简单地平均单词向量(为此，我们删除了停止单词，这只会增加噪音)。\n",
    "\n",
    "\n",
    "下面的代码对特征向量进行平均，构建在第2部分的代码之上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以调用这些函数来创建每个段落的平均向量。以下操作需要几分钟时间:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-980025a1769e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating average feature vecs for test reviews\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-95bfdf992c3d>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[1;34m(reviews, model, num_features)\u001b[0m\n\u001b[0;32m     17\u001b[0m        \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m        \u001b[1;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m        \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m        \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m        \u001b[1;31m# Increment the counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-cdf557ee2b65>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[1;34m(words, model, num_features)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Index2word is a list that contains the names of the words in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# the model's vocabulary. Convert it to a set, for speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mindex2word_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Loop over each word in the review and, if it is in the model's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\") \n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，使用平均段落向量来训练一个随机森林。注意，与第1部分中一样，我们只能使用标记的培训评审来培训模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainDataVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-38093612918f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting a random forest to labeled training data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrainDataVecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Test & extract results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainDataVecs' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\") \n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，这比碰运气的效果要好得多，但却比大言不话的效果差几个百分点。\n",
    "\n",
    "\n",
    "既然矢量的元素平均没有产生惊人的结果，也许我们可以用一种更聪明的方法来做?加权单词向量的标准方法是应用“tf-idf”权值，它度量给定单词在给定文档集合中的重要性。在Python中提取tf-idf权重的一种方法是使用scikit-learn的TfidfVectorizer，它的接口类似于我们在第1部分中使用的CountVectorizer。然而，当我们尝试以这种方式来加权我们的字向量时，我们发现在性能上没有实质性的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words to Paragraphs, Attempt 2: Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec创建语义相关的单词集群，因此另一种可能的方法是利用集群内单词的相似性。以这种方式分组向量称为“向量量化”。为此，我们首先需要找到词簇的中心，这可以通过使用K-Means之类的聚类算法来实现。\n",
    "\n",
    "\n",
    "在K- means中，我们需要设置的一个参数是“K”，即集群的数量。我们应该如何决定创建多少集群?试错结果表明，小的聚类，平均每个聚类只有5个单词左右，比有很多单词的大聚类结果更好。集群代码如下所示。我们使用scikit-learn来执行我们的K-Means。\n",
    "\n",
    "\n",
    "K-意味着聚类与大K可以非常缓慢;下面的代码在我的电脑上花了40多分钟。下面，我们围绕K-Means函数设置一个计时器，以查看需要多长时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f31dcaca4ca2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# average of 5 words per cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个单词的集群分配现在存储在idx中，原始Word2Vec模型中的词汇表仍然存储在model.index2word中。为了方便，我们把这些压缩到一个字典如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-402d718bce7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a Word / Index dictionary, mapping each vocabulary word to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# a cluster number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_centroid_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这有点抽象，让我们仔细看看我们的集群包含什么。您的集群可能不同，因为Word2Vec依赖于随机数种子。下面是一个循环，它打印出集群0到9的单词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-63-a6cc0cc8a7c4>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-63-a6cc0cc8a7c4>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print \"\\nCluster %d\" % cluster\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print(words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果非常有趣:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-ddaca8b2cd9f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-64-ddaca8b2cd9f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 0\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-5d82318982d4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-65-5d82318982d4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 1\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-66-2b0c1501d5ef>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-66-2b0c1501d5ef>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 2\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-81de173f4fd0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-81de173f4fd0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 3\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-a741daaae5af>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-68-a741daaae5af>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 4\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-6e6c286c0224>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-69-6e6c286c0224>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 5\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-70-3939f73a1396>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-70-3939f73a1396>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 6\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-71-373d775b6def>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-373d775b6def>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 7\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-72-caf8842782b4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-72-caf8842782b4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 8\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-73-05c41236b158>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-73-05c41236b158>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Cluster 9\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Cluster 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到集群的质量是不同的。有些是有意义的——集群3主要包含名称，集群6-8包含相关的形容词(集群6是我的最爱)。另一方面，集群5有点神秘:龙虾和鹿有什么共同点(除了是两种动物)?集群0更糟糕:顶层公寓和套房似乎属于一起，但它们似乎不属于苹果和护照。集群2包含…也许战争相关的单词?也许我们的算法对形容词最有效。\n",
    "\n",
    "\n",
    "无论如何，现在我们已经为每个单词分配了一个集群(或“重心”)，并且我们可以定义一个函数来将评论转换成重心。这就像一个词包，但使用语义相关的集群，而不是单独的词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的函数将为每个评论提供一个numpy数组，每个评论都有一些与集群数量相等的特性。最后，我们为我们的训练和测试集创建中心体包，然后训练一个随机森林并提取结果:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-c09a405aeeea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Pre-allocate an array for the training set bags of centroids (for speed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Transform the training set reviews into bags of centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\") \n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现上面的代码给出的结果与第1部分给出的结果大致相同(甚至更差)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
